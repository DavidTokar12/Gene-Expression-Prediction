{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.callbacks import Checkpoint\n",
    "from skorch.callbacks import EarlyStopping\n",
    "from skorch.callbacks import EpochScoring\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from gene_expression_prediction.data_loader import BigWigReader\n",
    "from gene_expression_prediction.data_loader import GeneReader\n",
    "from gene_expression_prediction.data_processor import CellLine\n",
    "from gene_expression_prediction.data_processor import FeatureNames\n",
    "from gene_expression_prediction.data_processor import ProcessedFeatures\n",
    "from gene_expression_prediction.data_processor import load_processed_features\n",
    "from gene_expression_prediction.data_processor import process_cell_line\n",
    "from gene_expression_prediction.data_processor import save_processed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.1 - Modeling Choices & Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required data paths have been successfully validated.\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "# Load your feature (bed and/or bigwig and/or fasta) and target files (tsv) here.\n",
    "# Decide which features to use for training. Feel free to process them however you need.\n",
    "\n",
    "# NOTE:\n",
    "# bed and bigwig files contain signals of all chromosomes (including sex chromosomes).\n",
    "# Training and validation split based on chromosomes has been done for you.\n",
    "# However, you can resplit the data in any way you want.\n",
    "\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "data_path = \"/workspaces/Gene-Expression-Prediction/data\"\n",
    "info_data = GeneReader(data_path)\n",
    "bigwig_data = BigWigReader(data_path)\n",
    "\n",
    "PROMOTER_WINDOW_SIZE = 5000\n",
    "PROMOTER_BIN_SIZE = 100\n",
    "\n",
    "DISTAL_WINDOW_SIZE = 50_000\n",
    "DISTAL_WINDOW_SIZE_BIN_SIZE=1000\n",
    "SAMPLE_N = None\n",
    "\n",
    "# Takes around 30 minutes to run\n",
    "# cell_line_x1 = process_cell_line(\n",
    "#     cell_line=CellLine.X1,\n",
    "#     gene_reader=info_data,\n",
    "#     bigwig_reader=bigwig_data,\n",
    "#     promoter_window_size=PROMOTER_WINDOW_SIZE,\n",
    "#     distal_window_size=DISTAL_WINDOW_SIZE,\n",
    "#     promoter_bin_size=PROMOTER_BIN_SIZE,\n",
    "#     distal_bin_size=DISTAL_WINDOW_SIZE_BIN_SIZE,\n",
    "#     sample_n=SAMPLE_N,\n",
    "# )\n",
    "# save_processed_features(\n",
    "#     cell_line_x1, \"/workspaces/Gene-Expression-Prediction/data/processed_data_x1\"\n",
    "# )\n",
    "cell_line_x1 = load_processed_features(\n",
    "    \"/workspaces/Gene-Expression-Prediction/data/processed_data_x1\"\n",
    ")\n",
    "\n",
    "\n",
    "# cell_line_x2 = process_cell_line(\n",
    "#     cell_line=CellLine.X2,\n",
    "#     gene_reader=info_data,\n",
    "#     bigwig_reader=bigwig_data,\n",
    "#     promoter_window_size=PROMOTER_WINDOW_SIZE,\n",
    "#     distal_window_size=DISTAL_WINDOW_SIZE,\n",
    "#     promoter_bin_size=PROMOTER_BIN_SIZE,\n",
    "#     distal_bin_size=DISTAL_WINDOW_SIZE_BIN_SIZE,\n",
    "#     sample_n=SAMPLE_N,\n",
    "# )\n",
    "# save_processed_features(\n",
    "#     cell_line_x2, \"/workspaces/Gene-Expression-Prediction/data/processed_data_x2\"\n",
    "# )\n",
    "cell_line_x2 = load_processed_features(\n",
    "    \"/workspaces/Gene-Expression-Prediction/data/processed_data_x2\"\n",
    ")\n",
    "\n",
    "\n",
    "# cell_line_x3 = process_cell_line(\n",
    "#     cell_line=CellLine.X3,\n",
    "#     gene_reader=info_data,\n",
    "#     bigwig_reader=bigwig_data,\n",
    "#     promoter_window_size=PROMOTER_WINDOW_SIZE,\n",
    "#     distal_window_size=DISTAL_WINDOW_SIZE,\n",
    "#     promoter_bin_size=PROMOTER_BIN_SIZE,\n",
    "#     distal_bin_size=DISTAL_WINDOW_SIZE_BIN_SIZE,\n",
    "#     sample_n=None,\n",
    "# )\n",
    "# save_processed_features(\n",
    "#     cell_line_x3, \"/workspaces/Gene-Expression-Prediction/data/processed_data_x3\"\n",
    "# )\n",
    "\n",
    "cell_line_x3 = load_processed_features(\n",
    "    \"/workspaces/Gene-Expression-Prediction/data/processed_data_x3\"\n",
    ")\n",
    "# ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining cell line features...\n",
      "\n",
      "============================================================\n",
      "VALIDATING COMBINED FEATURES ALIGNMENT\n",
      "============================================================\n",
      "Number of genes: 32568\n",
      "Promoter tensor shape: (32568, 190, 7, 2)\n",
      "Gene annotations shape: (32568, 7)\n",
      "Target expression shape: (32568,)\n",
      "All alignments validated successfully!\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def validate_combined_features(features: ProcessedFeatures):\n",
    "    \"\"\"Validates that all components of ProcessedFeatures are properly aligned.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"VALIDATING COMBINED FEATURES ALIGNMENT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    n_genes = len(features.gene_annotations)\n",
    "    gene_names = features.gene_annotations[\"gene_name\"].values\n",
    "\n",
    "    print(f\"Number of genes: {n_genes}\")\n",
    "    print(f\"Promoter tensor shape: {features.sequence_signal_tensor.shape}\")\n",
    "    print(f\"Gene annotations shape: {features.gene_annotations.shape}\")\n",
    "    print(\n",
    "        f\"Target expression shape: {features.target_expression.shape if features.target_expression is not None else 'None'}\"\n",
    "    )\n",
    "\n",
    "    if features.sequence_signal_tensor.shape[0] != n_genes:\n",
    "        print(\"ERROR: Promoter tensor length doesn't match gene annotations!\")\n",
    "        return False\n",
    "\n",
    "    if features.target_expression is not None:\n",
    "        if len(features.target_expression) != n_genes:\n",
    "            print(\"ERROR: Target expression length doesn't match gene annotations!\")\n",
    "            return False\n",
    "\n",
    "        target_genes = features.target_expression.index.values\n",
    "        if not np.array_equal(gene_names, target_genes):\n",
    "            print(\n",
    "                \"ERROR: Target expression index doesn't match gene_annotations order!\"\n",
    "            )\n",
    "            print(f\"  First 5 in annotations: {gene_names[:5]}\")\n",
    "            print(f\"  First 5 in targets: {target_genes[:5]}\")\n",
    "            return False\n",
    "\n",
    "    print(\"All alignments validated successfully!\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def combine_cell_lines(\n",
    "    cell_line_x1: ProcessedFeatures, cell_line_x2: ProcessedFeatures\n",
    ") -> ProcessedFeatures:\n",
    "    \"\"\"\n",
    "    Combines two ProcessedFeatures objects ensuring proper alignment.\n",
    "    \"\"\"\n",
    "    print(\"\\nCombining cell line features...\")\n",
    "\n",
    "    combined_tensor = np.concatenate(\n",
    "        [cell_line_x1.sequence_signal_tensor, cell_line_x2.sequence_signal_tensor],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    annotations_x1 = cell_line_x1.gene_annotations.copy()\n",
    "    annotations_x2 = cell_line_x2.gene_annotations.copy()\n",
    "    annotations_x1[\"gene_name\"] = annotations_x1[\"gene_name\"] + \"_x1\"\n",
    "    annotations_x2[\"gene_name\"] = annotations_x2[\"gene_name\"] + \"_x2\"\n",
    "    combined_annotations = pd.concat(\n",
    "        [annotations_x1, annotations_x2], ignore_index=True\n",
    "    )\n",
    "\n",
    "    targets_x1 = cell_line_x1.target_expression.copy()\n",
    "    targets_x2 = cell_line_x2.target_expression.copy()\n",
    "    targets_x1.index = targets_x1.index + \"_x1\"\n",
    "    targets_x2.index = targets_x2.index + \"_x2\"\n",
    "    combined_targets = pd.concat([targets_x1, targets_x2])\n",
    "\n",
    "    combined_targets = combined_targets.reindex(combined_annotations[\"gene_name\"])\n",
    "\n",
    "    combined_features = ProcessedFeatures(\n",
    "        gene_annotations=combined_annotations,\n",
    "        sequence_signal_tensor=combined_tensor,\n",
    "        target_expression=combined_targets,\n",
    "        n_upstream_bins=cell_line_x1.n_upstream_bins,\n",
    "        n_promoter_bins=cell_line_x1.n_promoter_bins,\n",
    "        n_downstream_bins=cell_line_x1.n_downstream_bins,\n",
    "        n_total_bins=cell_line_x1.n_total_bins,\n",
    "    )\n",
    "\n",
    "    if not validate_combined_features(combined_features):\n",
    "        raise ValueError(\"Combined features validation failed! Data is misaligned.\")\n",
    "\n",
    "    return combined_features\n",
    "\n",
    "\n",
    "combined_features = combine_cell_lines(cell_line_x1, cell_line_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.2 - Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Select the best model to predict gene expression from the obtained features in WP 1.1.\n",
    "# pytorch_dataset.py\n",
    "class GeneExpressionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for the gene expression prediction task.\n",
    "\n",
    "    This class takes the processed promoter signal tensor (with 2 channels: mean, max)\n",
    "    and target expression values, normalizes them, and prepares them for training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, processed_features: ProcessedFeatures):\n",
    "        if processed_features.target_expression is None:\n",
    "            raise ValueError(\"Target expression values are required for this dataset.\")\n",
    "\n",
    "        pt = torch.from_numpy(processed_features.sequence_signal_tensor).float()\n",
    "        \n",
    "        # Original shape: (N, Bins, Features, Channels) -> (N, F, C, B)\n",
    "        pt = pt.permute(0, 2, 3, 1).contiguous()\n",
    "        N, F, C, B = pt.shape\n",
    "\n",
    "        # Reshape to combine features and channels for Conv1d: (N, F * C, B)\n",
    "        self.promoter_tensor = pt.view(N, F * C, B)\n",
    "\n",
    "        # --- 2. Global Normalization per Channel (mean/max) ---\n",
    "        # The channels are interleaved: 0,2,4... are 'mean'; 1,3,5... are 'max'.\n",
    "        mean_channels = list(range(0, F * C, C))\n",
    "        max_channels = list(range(1, F * C, C))\n",
    "\n",
    "        mean_data = self.promoter_tensor[:, mean_channels, :]\n",
    "        max_data = self.promoter_tensor[:, max_channels, :]\n",
    "\n",
    "        # Calculate statistics for normalization\n",
    "        self.promoter_mean_mean = mean_data.mean().item()\n",
    "        self.promoter_mean_std = mean_data.std().item()\n",
    "        self.promoter_max_mean = max_data.mean().item()\n",
    "        self.promoter_max_std = max_data.std().item()\n",
    "\n",
    "        print(\"\\nPromoter normalization stats (global per channel):\")\n",
    "        print(f\"  Mean channel: μ={self.promoter_mean_mean:.3f}, std={self.promoter_mean_std:.3f}\")\n",
    "        print(f\"  Max channel:  μ={self.promoter_max_mean:.3f}, std={self.promoter_max_std:.3f}\")\n",
    "\n",
    "        # Apply Z-score normalization to each channel for each feature\n",
    "        for i in range(F):\n",
    "            if self.promoter_mean_std > 1e-8: # Avoid division by zero\n",
    "                self.promoter_tensor[:, i * C + 0, :] = (\n",
    "                    self.promoter_tensor[:, i * C + 0, :] - self.promoter_mean_mean\n",
    "                ) / self.promoter_mean_std\n",
    "\n",
    "            if self.promoter_max_std > 1e-8:\n",
    "                self.promoter_tensor[:, i * C + 1, :] = (\n",
    "                    self.promoter_tensor[:, i * C + 1, :] - self.promoter_max_mean\n",
    "                ) / self.promoter_max_std\n",
    "        \n",
    "        target_values = processed_features.target_expression.fillna(0.0).values\n",
    "        log_targets = np.log1p(target_values)\n",
    "\n",
    "        self.target_log_mean = log_targets.mean()\n",
    "        self.target_log_std = log_targets.std()\n",
    "        \n",
    "        normalized_targets = (log_targets - self.target_log_mean) / self.target_log_std\n",
    "        self.targets = torch.from_numpy(normalized_targets).float()\n",
    "        \n",
    "        print(\"\\nTarget normalization stats:\")\n",
    "        print(f\"  Original range: [{target_values.min():.1f}, {target_values.max():.1f}]\")\n",
    "        print(f\"  Log-transformed range: [{log_targets.min():.3f}, {log_targets.max():.3f}]\")\n",
    "        print(f\"  Final normalized range: [{self.targets.min():.3f}, {self.targets.max():.3f}]\")\n",
    "\n",
    "\n",
    "    def denormalize_targets(self, normalized_predictions):\n",
    "        \"\"\"Converts normalized model predictions back to the original gene expression scale.\"\"\"\n",
    "        if torch.is_tensor(normalized_predictions):\n",
    "            normalized_predictions = normalized_predictions.cpu().numpy()\n",
    "\n",
    "        log_predictions = (normalized_predictions * self.target_log_std) + self.target_log_mean\n",
    "        \n",
    "        original_scale = np.expm1(log_predictions)\n",
    "        \n",
    "        return np.clip(original_scale, 0, None)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.promoter_tensor[idx], self.targets[idx]\n",
    "    \n",
    "    \n",
    "class PromoterAttentionCNN(nn.Module):\n",
    "    def __init__(self, n_bins: int, n_features: int, n_channels: int):\n",
    "        super().__init__()\n",
    "        in_channels = n_features * n_channels\n",
    "        \n",
    "        # Deeper CNN tower with residual blocks\n",
    "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size=7, padding=\"same\")\n",
    "        self.norm1 = nn.GroupNorm(8, 64)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        # Residual block 1\n",
    "        self.res1_conv1 = nn.Conv1d(64, 64, kernel_size=5, padding=\"same\")\n",
    "        self.res1_norm1 = nn.GroupNorm(8, 64)\n",
    "        self.res1_conv2 = nn.Conv1d(64, 64, kernel_size=5, padding=\"same\")\n",
    "        self.res1_norm2 = nn.GroupNorm(8, 64)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=\"same\")\n",
    "        self.norm2 = nn.GroupNorm(8, 128)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        # Residual block 2\n",
    "        self.res2_conv1 = nn.Conv1d(128, 128, kernel_size=3, padding=\"same\")\n",
    "        self.res2_norm1 = nn.GroupNorm(8, 128)\n",
    "        self.res2_conv2 = nn.Conv1d(128, 128, kernel_size=3, padding=\"same\")\n",
    "        self.res2_norm2 = nn.GroupNorm(8, 128)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        \n",
    "        # Additional conv layer\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=\"same\")\n",
    "        self.norm3 = nn.GroupNorm(8, 256)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.gap = nn.AdaptiveAvgPool1d(32)\n",
    "        \n",
    "        # Multi-head attention with more heads\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            embed_dim=256, num_heads=8, dropout=0.1, batch_first=True  # 8 heads instead of 4\n",
    "        )\n",
    "        self.attn_norm = nn.LayerNorm(256)\n",
    "        \n",
    "        # Prediction head\n",
    "        self.fc1 = nn.Linear(256 * 32, 512)\n",
    "        self.fc_norm = nn.LayerNorm(512)\n",
    "        self.fc_dropout = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc_dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "    \n",
    "    def _residual_block(self, x, conv1, norm1, conv2, norm2):\n",
    "        \"\"\"Residual block with skip connection.\"\"\"\n",
    "        identity = x\n",
    "        out = F.relu(norm1(conv1(x)))\n",
    "        out = norm2(conv2(out))\n",
    "        out += identity  # Skip connection\n",
    "        return F.relu(out)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initial conv\n",
    "        x = F.relu(self.norm1(self.conv1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Residual block 1\n",
    "        x = self._residual_block(x, self.res1_conv1, self.res1_norm1, \n",
    "                                 self.res1_conv2, self.res1_norm2)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # Conv 2\n",
    "        x = F.relu(self.norm2(self.conv2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Residual block 2\n",
    "        x = self._residual_block(x, self.res2_conv1, self.res2_norm1,\n",
    "                                 self.res2_conv2, self.res2_norm2)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        # Conv 3\n",
    "        x = F.relu(self.norm3(self.conv3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # GAP\n",
    "        x = self.gap(x)  # (batch, 256, 32)\n",
    "        \n",
    "        # Self-attention\n",
    "        x_t = x.transpose(1, 2)\n",
    "        attn_out, _ = self.self_attn(x_t, x_t, x_t)\n",
    "        x_t = self.attn_norm(x_t + attn_out)  # Residual\n",
    "        \n",
    "        # Flatten and predict\n",
    "        h = torch.flatten(x_t, 1)\n",
    "        h = F.relu(self.fc_norm(self.fc1(h)))\n",
    "        h = self.fc_dropout(h)\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = self.fc_dropout2(h)\n",
    "        return self.fc3(h).squeeze(1)\n",
    "# ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_epoch_scorer(net, dataset_valid, y=None):\n",
    "    \"\"\"\n",
    "    Custom skorch scorer to calculate Spearman correlation on the validation set.\n",
    "\n",
    "    This function correctly handles skorch's behavior by ignoring the potentially\n",
    "    incomplete 'y' parameter and reconstructing the full y_true array from the\n",
    "    provided validation dataset.\n",
    "    \"\"\"\n",
    "    y_pred = net.predict(dataset_valid).ravel()\n",
    "    y_true = np.array([y_i.item() for _, y_i in dataset_valid]).ravel()\n",
    "\n",
    "    correlation, _ = spearmanr(y_true, y_pred)\n",
    "\n",
    "    # This is a necessary sanity check, not error hiding. spearmanr can return\n",
    "    # NaN if all predictions are identical\n",
    "    if np.isnan(correlation):\n",
    "        return 0.0\n",
    "\n",
    "    return float(correlation)\n",
    "\n",
    "\n",
    "def validate_dataset(dataset):\n",
    "    \"\"\"Validates dataset for NaN, Inf, and prints value ranges.\"\"\"\n",
    "    print(\"\\n=== Data Validation ===\")\n",
    "\n",
    "    has_nan = torch.isnan(dataset.promoter_tensor).any()\n",
    "    has_inf = torch.isinf(dataset.promoter_tensor).any()\n",
    "    print(f\"Promoter - NaN: {has_nan}, Inf: {has_inf}\")\n",
    "    print(\n",
    "        f\"Promoter range: [{dataset.promoter_tensor.min():.3f}, {dataset.promoter_tensor.max():.3f}]\"\n",
    "    )\n",
    "\n",
    "    has_nan = torch.isnan(dataset.targets).any()\n",
    "    has_inf = torch.isinf(dataset.targets).any()\n",
    "    print(f\"Targets - NaN: {has_nan}, Inf: {has_inf}\")\n",
    "    print(f\"Targets range: [{dataset.targets.min():.3f}, {dataset.targets.max():.3f}]\")\n",
    "    print(\n",
    "        f\"Targets with value 0.0: {(dataset.targets == 0.0).sum()} / {len(dataset.targets)}\"\n",
    "    )\n",
    "    print(\"=====================\\n\")\n",
    "\n",
    "\n",
    "def train_with_skorch(\n",
    "    full_ds: Dataset,\n",
    "    model: nn.Module,\n",
    "    *,\n",
    "    device: torch.device,\n",
    "    batch_size: int = 64,\n",
    "    max_epochs: int = 10_000,\n",
    "    learning_rate: float = 1e-3,\n",
    "    num_workers: int = 0,\n",
    "    patience: int = 10,\n",
    "    min_delta: float = 1e-4,\n",
    "    checkpoint_dir: str = \"checkpoints\",\n",
    "    monitor_name: str = \"valid_spearman\",\n",
    "):\n",
    "    N = len(full_ds)\n",
    "    indices = list(range(N))\n",
    "    train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_ds = Subset(full_ds, train_idx)\n",
    "    valid_ds = Subset(full_ds, val_idx)\n",
    "\n",
    "    pin_memory = device.type == \"cuda\"\n",
    "    callbacks = [\n",
    "        EpochScoring(\n",
    "            spearman_epoch_scorer,\n",
    "            lower_is_better=False,\n",
    "            name=monitor_name,\n",
    "            use_caching=False,\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=monitor_name,\n",
    "            patience=patience,\n",
    "            threshold=min_delta,\n",
    "            lower_is_better=False,\n",
    "        ),\n",
    "        Checkpoint(\n",
    "            dirname=checkpoint_dir,\n",
    "            monitor=f\"{monitor_name}_best\",\n",
    "            f_params=\"best_model.pt\",\n",
    "        ),\n",
    "        LRScheduler(\n",
    "            policy=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "            mode=\"max\",\n",
    "            factor=0.5,\n",
    "            patience=max(2, patience // 2),\n",
    "            monitor=monitor_name,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    net = NeuralNetRegressor(\n",
    "        model,\n",
    "        criterion=nn.MSELoss,\n",
    "        optimizer=optim.AdamW,\n",
    "        optimizer__lr=learning_rate,\n",
    "        optimizer__weight_decay=1e-4,\n",
    "        max_epochs=max_epochs,\n",
    "        batch_size=batch_size,\n",
    "        device=device.type,\n",
    "        train_split=predefined_split(valid_ds),\n",
    "        callbacks=callbacks,\n",
    "        iterator_train__num_workers=num_workers,\n",
    "        iterator_valid__num_workers=num_workers,\n",
    "        iterator_train__pin_memory=pin_memory,\n",
    "        iterator_valid__pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    print(f\"Combined dataset size: {N}\")\n",
    "    print(f\"Training set size:   {len(train_idx)}\")\n",
    "    print(f\"Validation set size: {len(val_idx)}\")\n",
    "\n",
    "    net.fit(train_ds, y=None)\n",
    "\n",
    "    from pathlib import Path\n",
    "\n",
    "    net.load_params(f_params=Path(checkpoint_dir) / \"best_model.pt\")\n",
    "    best_torch_model = net.module_\n",
    "    return net, best_torch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Creating dataset...\n",
      "\n",
      "Promoter normalization stats (global per channel):\n",
      "  Mean channel: μ=1.015, std=5.113\n",
      "  Max channel:  μ=2.123, std=8.314\n",
      "\n",
      "Target normalization stats:\n",
      "  Original range: [0.0, 19519.8]\n",
      "  Log-transformed range: [0.000, 9.879]\n",
      "  Final normalized range: [-0.653, 4.481]\n",
      "\n",
      "=== Data Validation ===\n",
      "Promoter - NaN: False, Inf: False\n",
      "Promoter range: [-0.255, 94.493]\n",
      "Targets - NaN: False, Inf: False\n",
      "Targets range: [-0.653, 4.481]\n",
      "Targets with value 0.0: 0 / 32568\n",
      "=====================\n",
      "\n",
      "Starting training...\n",
      "Combined dataset size: 32568\n",
      "Training set size:   26054\n",
      "Validation set size: 6514\n",
      "  epoch    train_loss    valid_loss    valid_spearman    cp      lr      dur\n",
      "-------  ------------  ------------  ----------------  ----  ------  -------\n",
      "      1        \u001b[36m0.5090\u001b[0m        \u001b[32m0.4296\u001b[0m            \u001b[35m0.7513\u001b[0m     +  0.0001  43.0261\n",
      "      2        \u001b[36m0.4347\u001b[0m        0.4327            \u001b[35m0.7615\u001b[0m     +  0.0001  42.5475\n",
      "      3        \u001b[36m0.4051\u001b[0m        \u001b[32m0.3786\u001b[0m            \u001b[35m0.7736\u001b[0m     +  0.0001  42.5057\n",
      "      4        \u001b[36m0.3837\u001b[0m        0.4034            \u001b[35m0.7790\u001b[0m     +  0.0001  43.0113\n",
      "      5        \u001b[36m0.3635\u001b[0m        0.3816            0.7773        0.0001  42.6311\n",
      "      6        \u001b[36m0.3452\u001b[0m        0.3852            \u001b[35m0.7792\u001b[0m     +  0.0001  44.3941\n",
      "      7        \u001b[36m0.3404\u001b[0m        0.3935            \u001b[35m0.7818\u001b[0m     +  0.0001  44.5433\n",
      "      8        \u001b[36m0.3280\u001b[0m        \u001b[32m0.3734\u001b[0m            \u001b[35m0.7847\u001b[0m     +  0.0001  46.5802\n",
      "      9        \u001b[36m0.3223\u001b[0m        \u001b[32m0.3617\u001b[0m            \u001b[35m0.7899\u001b[0m     +  0.0001  45.7877\n",
      "     10        \u001b[36m0.3167\u001b[0m        \u001b[32m0.3512\u001b[0m            0.7857        0.0001  47.4862\n",
      "     11        \u001b[36m0.3047\u001b[0m        0.3604            0.7836        0.0001  45.5430\n",
      "     12        \u001b[36m0.3012\u001b[0m        \u001b[32m0.3345\u001b[0m            0.7891        0.0001  45.8159\n",
      "     13        \u001b[36m0.2941\u001b[0m        0.3448            \u001b[35m0.7904\u001b[0m     +  0.0001  42.3939\n",
      "     14        \u001b[36m0.2868\u001b[0m        0.3460            0.7890        0.0001  48.4247\n",
      "     15        \u001b[36m0.2815\u001b[0m        0.3375            \u001b[35m0.7906\u001b[0m     +  0.0001  46.0886\n",
      "     16        \u001b[36m0.2706\u001b[0m        0.3443            0.7898        0.0001  47.5344\n",
      "     17        \u001b[36m0.2652\u001b[0m        \u001b[32m0.3320\u001b[0m            \u001b[35m0.7924\u001b[0m     +  0.0001  45.1311\n",
      "     18        \u001b[36m0.2577\u001b[0m        \u001b[32m0.3269\u001b[0m            0.7916        0.0001  48.4859\n",
      "     19        \u001b[36m0.2571\u001b[0m        0.3292            0.7917        0.0001  45.7431\n",
      "     20        \u001b[36m0.2462\u001b[0m        \u001b[32m0.3215\u001b[0m            0.7917        0.0001  46.3741\n",
      "     21        \u001b[36m0.2421\u001b[0m        0.3322            0.7920        0.0001  47.3760\n",
      "     22        \u001b[36m0.2356\u001b[0m        0.3346            \u001b[35m0.7925\u001b[0m     +  0.0001  2066.7690\n",
      "     23        \u001b[36m0.2339\u001b[0m        0.3336            0.7924        0.0001  43.4788\n",
      "     24        \u001b[36m0.2258\u001b[0m        0.3371            \u001b[35m0.7948\u001b[0m     +  0.0001  42.4681\n",
      "     25        \u001b[36m0.2213\u001b[0m        0.3580            0.7939        0.0001  45.4945\n",
      "     26        \u001b[36m0.2133\u001b[0m        0.3437            0.7934        0.0001  42.5422\n",
      "     27        \u001b[36m0.2127\u001b[0m        0.3797            0.7904        0.0001  43.6548\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "PROMOTER_WINDOW_SIZE = 5000\n",
    "PROMOTER_BIN_SIZE = 100\n",
    "N_BINS = (2 * PROMOTER_WINDOW_SIZE) // PROMOTER_BIN_SIZE\n",
    "N_FEATURES = len(FeatureNames)\n",
    "N_CHANNELS = 2 # mean, max\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_WORKERS = 2\n",
    "MAX_EPOCHS = 100\n",
    "PATIENCE = 15\n",
    "MIN_DELTA = 1e-4\n",
    "MODEL_SAVE_PATH = \"./best_promoter_model.pth\"\n",
    "\n",
    "print(\"Creating dataset...\")\n",
    "full_ds = GeneExpressionDataset(combined_features)\n",
    "validate_dataset(full_ds)\n",
    "\n",
    "model = PromoterAttentionCNN(\n",
    "    n_bins=N_BINS,\n",
    "    n_features=N_FEATURES,\n",
    "    n_channels=N_CHANNELS,\n",
    ")\n",
    "\n",
    "# Run Training\n",
    "print(\"Starting training...\")\n",
    "net, best_model = train_with_skorch(\n",
    "    full_ds=full_ds,\n",
    "    model=model,\n",
    "    device=DEVICE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    patience=PATIENCE,\n",
    "    min_delta=MIN_DELTA,\n",
    "    checkpoint_dir=\"checkpoints_promoter_only\",\n",
    ")\n",
    "\n",
    "torch.save(best_model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"\\nSaved best model to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.3 - Prediction on Test Data (Evaluation Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ProcessedFeatures' object has no attribute 'distal_peak_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m pred = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      3\u001b[39m test_genes = cell_line_x3.gene_annotations[\u001b[33m\"\u001b[39m\u001b[33mgene_name\u001b[39m\u001b[33m\"\u001b[39m].values\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m _train_cols = \u001b[38;5;28mlist\u001b[39m(\u001b[43mcombined_features\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistal_peak_features\u001b[49m.columns)\n\u001b[32m      6\u001b[39m cell_line_x3.distal_peak_features = cell_line_x3.distal_peak_features.reindex(\n\u001b[32m      7\u001b[39m     cell_line_x3.gene_annotations[\u001b[33m\"\u001b[39m\u001b[33mgene_name\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m )[_train_cols].fillna(\u001b[32m0.0\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_PredDS\u001b[39;00m(Dataset):\n",
      "\u001b[31mAttributeError\u001b[39m: 'ProcessedFeatures' object has no attribute 'distal_peak_features'"
     ]
    }
   ],
   "source": [
    "pred = None\n",
    "\n",
    "test_genes = cell_line_x3.gene_annotations[\"gene_name\"].values\n",
    "\n",
    "_train_cols = list(combined_features.distal_peak_features.columns)\n",
    "cell_line_x3.distal_peak_features = cell_line_x3.distal_peak_features.reindex(\n",
    "    cell_line_x3.gene_annotations[\"gene_name\"]\n",
    ")[_train_cols].fillna(0.0)\n",
    "\n",
    "\n",
    "\n",
    "# Get the training dataset to extract normalization parameters\n",
    "full_train_ds = GeneExpressionDataset(combined_features)\n",
    "\n",
    "# Create test dataset with proper normalization\n",
    "_test_ds = _PredDS(cell_line_x3, full_train_ds)\n",
    "\n",
    "\n",
    "class _SkorchModule(nn.Module):\n",
    "    def __init__(self, core):\n",
    "        super().__init__()\n",
    "        self.core = core\n",
    "\n",
    "    def forward(self, X):\n",
    "        promoter_x, distal_x = X\n",
    "        return self.core(promoter_x, distal_x)\n",
    "\n",
    "\n",
    "# Create model\n",
    "_core = HybridCNN(\n",
    "    n_bins=N_BINS,\n",
    "    n_features=N_FEATURES,\n",
    "    n_channels=N_CHANNELS,\n",
    "    n_distal_features=len(_train_cols),\n",
    ")\n",
    "\n",
    "_net = NeuralNetRegressor(\n",
    "    _SkorchModule(_core),\n",
    "    device=DEVICE.type,\n",
    "    batch_size=BATCH_SIZE * 2,\n",
    ")\n",
    "\n",
    "_net.initialize()\n",
    "_net.load_params(f_params=\"checkpoints/best_model.pt\")\n",
    "\n",
    "# Predict and clip negatives to zero\n",
    "print(\"Making predictions...\")\n",
    "pred = _net.predict(_test_ds).ravel()\n",
    "pred = np.clip(pred, 0, None)\n",
    "\n",
    "print(f\"Predictions shape: {pred.shape}\")\n",
    "print(f\"Predictions range: [{pred.min():.3f}, {pred.max():.3f}]\")\n",
    "\n",
    "# Check if \"pred\" meets the specified constraints\n",
    "assert isinstance(pred, np.ndarray), \"Prediction array must be a numpy array\"\n",
    "assert np.issubdtype(pred.dtype, np.number), \"Prediction array must be numeric\"\n",
    "assert pred.shape[0] == len(test_genes), (\n",
    "    \"Each gene should have a unique predicted expression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Predictions in the Required Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: /workspaces/Gene-Expression-Prediction/data/output/Tokar_David_Project1.zip\n",
      "\n",
      "Preview of the first 5 rows of the submission file:\n",
      "    gene_name  gex_predicted\n",
      "0       CAPN9       0.000000\n",
      "1        ILF2       0.674938\n",
      "2  ST6GALNAC5       0.000000\n",
      "3  MROH7-TTC4       0.000000\n",
      "4        AGO4       0.000000\n"
     ]
    }
   ],
   "source": [
    "# Store predictions in a ZIP.\n",
    "# Upload this zip on the project website under \"Your submission\".\n",
    "# Zip this notebook along with the conda environment (and README, optional) and upload this under \"Your code\".\n",
    "save_dir = \"/workspaces/Gene-Expression-Prediction/data/output\"\n",
    "file_name = \"gex_predicted.csv\"  # PLEASE DO NOT CHANGE THIS\n",
    "zip_name = \"Tokar_David_Project1.zip\"\n",
    "save_path = f\"{save_dir}/{zip_name}\"\n",
    "compression_options = {\"method\": \"zip\", \"archive_name\": file_name}\n",
    "\n",
    "submission_df = pd.DataFrame({\"gene_name\": test_genes, \"gex_predicted\": pred})\n",
    "\n",
    "compression_options = {\"method\": \"zip\", \"archive_name\": file_name}\n",
    "\n",
    "submission_df.to_csv(save_path, index=False, compression=compression_options)\n",
    "print(f\"File saved to: {save_path}\")\n",
    "print(\"\\nPreview of the first 5 rows of the submission file:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
