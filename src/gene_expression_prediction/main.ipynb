{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.callbacks import Checkpoint\n",
    "from skorch.callbacks import EarlyStopping\n",
    "from skorch.callbacks import EpochScoring\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from gene_expression_prediction.data_processor import FeatureNames\n",
    "from gene_expression_prediction.data_processor import ProcessedFeatures\n",
    "from gene_expression_prediction.data_processor import process_all_cell_lines\n",
    "from gene_expression_prediction.data_processor import save_processed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.1 - Modeling Choices & Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GENE EXPRESSION FEATURE PROCESSING - ENFORMER STYLE\n",
      "======================================================================\n",
      "Data path:     /workspaces/Gene-Expression-Prediction/data\n",
      "Window size:   196,608 bp (±98,304 bp from TSS)\n",
      "Bin size:      128 bp\n",
      "Total bins:    1536\n",
      "Sample size:   All genes\n",
      "Save to disk:  True\n",
      "======================================================================\n",
      "\n",
      "Loading data readers...\n",
      "All required data paths have been successfully validated.\n",
      "\n",
      "======================================================================\n",
      "PROCESSING CELL LINE X1 (Training)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Uniform Binning Configuration (Enformer-style):\n",
      "======================================================================\n",
      "  Target window size:   196,608 bp\n",
      "  Bin size:             128 bp\n",
      "  Computed n_bins:      1,536 (window_size // bin_size)\n",
      "  Actual coverage:      196,608 bp (n_bins * bin_size)\n",
      "  Window center:        TSS ±98,304 bp\n",
      "  Number of features:   7\n",
      "  Number of genes:      16,284\n",
      "  Tensor shape:         (16,284, 1,536, 7)\n",
      "  Memory (est.):        0.70 GB\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdfe8d348f34f3d85dfe73e999b8611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing X1:   0%|          | 0/16284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Info: 25 genes were padded with zeros due to proximity to chromosome ends.\n",
      "  (Example padded genes: ['TUBB8', 'FAM110C', 'ZNF595', 'DUX4', 'ZNF268'])\n",
      "✓ Saved processed features to: /workspaces/Gene-Expression-Prediction/data/processed_data_x1_enformer\n",
      "\n",
      "======================================================================\n",
      "PROCESSING CELL LINE X2 (Validation)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Uniform Binning Configuration (Enformer-style):\n",
      "======================================================================\n",
      "  Target window size:   196,608 bp\n",
      "  Bin size:             128 bp\n",
      "  Computed n_bins:      1,536 (window_size // bin_size)\n",
      "  Actual coverage:      196,608 bp (n_bins * bin_size)\n",
      "  Window center:        TSS ±98,304 bp\n",
      "  Number of features:   7\n",
      "  Number of genes:      16,284\n",
      "  Tensor shape:         (16,284, 1,536, 7)\n",
      "  Memory (est.):        0.70 GB\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7700479b9ae1419a8b840fbb5f042972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing X2:   0%|          | 0/16284 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Info: 25 genes were padded with zeros due to proximity to chromosome ends.\n",
      "  (Example padded genes: ['TUBB8', 'FAM110C', 'ZNF595', 'DUX4', 'ZNF268'])\n",
      "✓ Saved processed features to: /workspaces/Gene-Expression-Prediction/data/processed_data_x2_enformer\n",
      "\n",
      "======================================================================\n",
      "PROCESSING CELL LINE X3 (Test - using all genes)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Uniform Binning Configuration (Enformer-style):\n",
      "======================================================================\n",
      "  Target window size:   196,608 bp\n",
      "  Bin size:             128 bp\n",
      "  Computed n_bins:      1,536 (window_size // bin_size)\n",
      "  Actual coverage:      196,608 bp (n_bins * bin_size)\n",
      "  Window center:        TSS ±98,304 bp\n",
      "  Number of features:   7\n",
      "  Number of genes:      1,984\n",
      "  Tensor shape:         (1,984, 1,536, 7)\n",
      "  Memory (est.):        0.09 GB\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d8ced507fe46939f6f05da09363151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing X3:   0%|          | 0/1984 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Info: 2 genes were padded with zeros due to proximity to chromosome ends.\n",
      "  (Example padded genes: ['PGBD2', 'OR4F5'])\n",
      "✓ Saved processed features to: /workspaces/Gene-Expression-Prediction/data/processed_data_x3_enformer\n",
      "\n",
      "======================================================================\n",
      "✓ ALL CELL LINES PROCESSED SUCCESSFULLY!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "# Load your feature (bed and/or bigwig and/or fasta) and target files (tsv) here.\n",
    "# Decide which features to use for training. Feel free to process them however you need.\n",
    "\n",
    "# NOTE:\n",
    "# bed and bigwig files contain signals of all chromosomes (including sex chromosomes).\n",
    "# Training and validation split based on chromosomes has been done for you.\n",
    "# However, you can resplit the data in any way you want.\n",
    "\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "DATA_PATH = \"/workspaces/Gene-Expression-Prediction/data\"\n",
    "\n",
    "WINDOW_SIZE = 196_608\n",
    "BIN_SIZE = 128\n",
    "SAMPLE_N = None  # Set to e.g., 100 for quick testing, None for full dataset\n",
    "\n",
    "\n",
    "cell_line_x1, cell_line_x2, cell_line_x3 = process_all_cell_lines(\n",
    "    data_path=DATA_PATH,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    bin_size=BIN_SIZE,\n",
    "    sample_n=SAMPLE_N,\n",
    "    save_to_disk=True,\n",
    "    output_dir=DATA_PATH,\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell_line_x1 = load_processed_features(\n",
    "#     \"/workspaces/Gene-Expression-Prediction/data/processed_data_x1\"\n",
    "# )\n",
    "# cell_line_x2 = load_processed_features(\n",
    "#     \"/workspaces/Gene-Expression-Prediction/data/processed_data_x2\"\n",
    "# )\n",
    "# cell_line_x3 = load_processed_features(\n",
    "#     \"/workspaces/Gene-Expression-Prediction/data/processed_data_x3\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining cell line features...\n",
      "\n",
      "============================================================\n",
      "VALIDATING COMBINED FEATURES ALIGNMENT\n",
      "============================================================\n",
      "Number of genes: 32568\n",
      "Promoter tensor shape: (32568, 1536, 7)\n",
      "Gene annotations shape: (32568, 7)\n",
      "Target expression shape: (32568,)\n",
      "All alignments validated successfully!\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def validate_combined_features(features: ProcessedFeatures):\n",
    "    \"\"\"Validates that all components of ProcessedFeatures are properly aligned.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"VALIDATING COMBINED FEATURES ALIGNMENT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    n_genes = len(features.gene_annotations)\n",
    "    gene_names = features.gene_annotations[\"gene_name\"].values\n",
    "\n",
    "    print(f\"Number of genes: {n_genes}\")\n",
    "    print(f\"Promoter tensor shape: {features.sequence_signal_tensor.shape}\")\n",
    "    print(f\"Gene annotations shape: {features.gene_annotations.shape}\")\n",
    "    print(\n",
    "        f\"Target expression shape: {features.target_expression.shape if features.target_expression is not None else 'None'}\"\n",
    "    )\n",
    "\n",
    "    if features.sequence_signal_tensor.shape[0] != n_genes:\n",
    "        print(\"ERROR: Promoter tensor length doesn't match gene annotations!\")\n",
    "        return False\n",
    "\n",
    "    if features.target_expression is not None:\n",
    "        if len(features.target_expression) != n_genes:\n",
    "            print(\"ERROR: Target expression length doesn't match gene annotations!\")\n",
    "            return False\n",
    "\n",
    "        target_genes = features.target_expression.index.values\n",
    "        if not np.array_equal(gene_names, target_genes):\n",
    "            print(\n",
    "                \"ERROR: Target expression index doesn't match gene_annotations order!\"\n",
    "            )\n",
    "            print(f\"  First 5 in annotations: {gene_names[:5]}\")\n",
    "            print(f\"  First 5 in targets: {target_genes[:5]}\")\n",
    "            return False\n",
    "\n",
    "    print(\"All alignments validated successfully!\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def combine_cell_lines(\n",
    "    cell_line_x1: ProcessedFeatures, cell_line_x2: ProcessedFeatures\n",
    ") -> ProcessedFeatures:\n",
    "    \"\"\"\n",
    "    Combines two ProcessedFeatures objects ensuring proper alignment.\n",
    "    \"\"\"\n",
    "    print(\"\\nCombining cell line features...\")\n",
    "\n",
    "    combined_tensor = np.concatenate(\n",
    "        [cell_line_x1.sequence_signal_tensor, cell_line_x2.sequence_signal_tensor],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    annotations_x1 = cell_line_x1.gene_annotations.copy()\n",
    "    annotations_x2 = cell_line_x2.gene_annotations.copy()\n",
    "    annotations_x1[\"gene_name\"] = annotations_x1[\"gene_name\"] + \"_x1\"\n",
    "    annotations_x2[\"gene_name\"] = annotations_x2[\"gene_name\"] + \"_x2\"\n",
    "    combined_annotations = pd.concat(\n",
    "        [annotations_x1, annotations_x2], ignore_index=True\n",
    "    )\n",
    "\n",
    "    targets_x1 = cell_line_x1.target_expression.copy()\n",
    "    targets_x2 = cell_line_x2.target_expression.copy()\n",
    "    targets_x1.index = targets_x1.index + \"_x1\"\n",
    "    targets_x2.index = targets_x2.index + \"_x2\"\n",
    "    combined_targets = pd.concat([targets_x1, targets_x2])\n",
    "\n",
    "    combined_targets = combined_targets.reindex(combined_annotations[\"gene_name\"])\n",
    "\n",
    "    combined_features = ProcessedFeatures(\n",
    "        gene_annotations=combined_annotations,\n",
    "        sequence_signal_tensor=combined_tensor,\n",
    "        window_size=cell_line_x1.window_size,\n",
    "        bin_size=cell_line_x1.bin_size,\n",
    "        n_bins=cell_line_x1.n_bins,\n",
    "        target_expression=combined_targets,\n",
    "    )\n",
    "\n",
    "    if not validate_combined_features(combined_features):\n",
    "        raise ValueError(\"Combined features validation failed! Data is misaligned.\")\n",
    "\n",
    "    return combined_features\n",
    "\n",
    "\n",
    "combined_features = combine_cell_lines(cell_line_x1, cell_line_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.2 - Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Select the best model to predict gene expression from the obtained features in WP 1.1.\n",
    "# pytorch_dataset.py\n",
    "\n",
    "class GeneExpressionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for gene expression prediction.\n",
    "\n",
    "    Handles a 3D input tensor (N, B, F) from ProcessedFeatures.\n",
    "    Implements `asinh` transform and per-feature z-score normalization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, processed_features: ProcessedFeatures, normalize_params=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            processed_features: ProcessedFeatures object.\n",
    "                                Assumes sequence_signal_tensor is (N_genes, N_bins, N_features).\n",
    "            normalize_params: Optional dict with normalization params from training.\n",
    "        \"\"\"\n",
    "        is_training = normalize_params is None\n",
    "        requires_targets = processed_features.target_expression is not None\n",
    "\n",
    "        if is_training and not requires_targets:\n",
    "            raise ValueError(\"Target expression required for training dataset.\")\n",
    "\n",
    "        # --- INPUT TENSOR PROCESSING ---\n",
    "        # 1. Load 3D tensor: (N_genes, N_bins, N_features)\n",
    "        #    (e.g., N, 1562, 7)\n",
    "        pt = torch.from_numpy(processed_features.sequence_signal_tensor).float()\n",
    "\n",
    "        # 2. Permute to (N, F, B) for easier processing\n",
    "        #    (N, 7, 1562)\n",
    "        pt = pt.permute(0, 2, 1).contiguous()\n",
    "        N, F, B = pt.shape # F=7 (features), B=1562 (bins)\n",
    "\n",
    "        # 3.Apply asinh transformation\n",
    "        # This stabilizes variance before normalization\n",
    "        pt = torch.asinh(pt)\n",
    "\n",
    "        if is_training:\n",
    "            # --- TRAINING MODE: Compute normalization ---\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"TRAINING MODE: Computing per-feature normalization\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            # Compute stats *per-feature*\n",
    "            # pt shape is (N, F, B)\n",
    "            # We compute stats over N and B dimensions\n",
    "            self.feature_means = pt.mean(dim=(0, 2)) # Shape (F,)\n",
    "            self.feature_stds = pt.std(dim=(0, 2))   # Shape (F,)\n",
    "            \n",
    "            # Add a small epsilon to stds to prevent division by zero\n",
    "            self.feature_stds[self.feature_stds < 1e-8] = 1e-8\n",
    "            \n",
    "            print(f\"Computed stats for {F} features (1 channel: mean)\")\n",
    "            print(f\"  Mean stats tensor shape: {self.feature_means.shape}\")\n",
    "            print(f\"  Std stats tensor shape:  {self.feature_stds.shape}\")\n",
    "            \n",
    "            print(\"\\nTarget normalization:\")\n",
    "            target_values = processed_features.target_expression.fillna(0.0).values\n",
    "            log_targets = np.log1p(target_values)\n",
    "\n",
    "            self.target_log_mean = log_targets.mean()\n",
    "            self.target_log_std = log_targets.std()\n",
    "            \n",
    "            normalized_targets = (\n",
    "                log_targets - self.target_log_mean\n",
    "            ) / self.target_log_std\n",
    "            self.targets = torch.from_numpy(normalized_targets).float()\n",
    "            print(f\"  log1p mean: {self.target_log_mean:.3f}, std: {self.target_log_std:.3f}\")\n",
    "\n",
    "        else:\n",
    "            # --- TEST MODE: Apply training normalization ---\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"TEST MODE: Applying training normalization parameters\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            self.feature_means = normalize_params[\"feature_means\"]\n",
    "            self.feature_stds = normalize_params[\"feature_stds\"]\n",
    "            self.target_log_mean = normalize_params[\"target_log_mean\"]\n",
    "            self.target_log_std = normalize_params[\"target_log_std\"]\n",
    "            \n",
    "            print(f\"Loaded stats for {self.feature_means.shape[0]} features\")\n",
    "            self.targets = torch.zeros(N) # Dummy targets for test set\n",
    "\n",
    "        # --- APPLY PER-FEATURE NORMALIZATION ---\n",
    "        # Use broadcasting to normalize\n",
    "        # pt shape:        (N, F, B)\n",
    "        # means/stds shape:    (F,)\n",
    "        # Unsqueeze to:      (1, F, 1) to broadcast over N and B\n",
    "        \n",
    "        mean_tensor = self.feature_means.unsqueeze(0).unsqueeze(2)\n",
    "        std_tensor = self.feature_stds.unsqueeze(0).unsqueeze(2)\n",
    "        \n",
    "        pt = (pt - mean_tensor) / std_tensor\n",
    "\n",
    "        # --- FINAL TENSOR ---\n",
    "        # The tensor is already in the correct (N, F, B) shape\n",
    "        self.promoter_tensor = pt\n",
    "\n",
    "        print(f\"\\nDataset size: {N} samples\")\n",
    "        print(f\"Tensor shape: {tuple(self.promoter_tensor.shape)}\")\n",
    "        print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    def get_normalization_params(self):\n",
    "        \"\"\" Returns normalization parameters for use with test set. \"\"\"\n",
    "        return {\n",
    "            \"feature_means\": self.feature_means,\n",
    "            \"feature_stds\": self.feature_stds,\n",
    "            \"target_log_mean\": self.target_log_mean,\n",
    "            \"target_log_std\": self.target_log_std,\n",
    "        }\n",
    "\n",
    "    def denormalize_targets(self, normalized_predictions):\n",
    "        \"\"\"\n",
    "        Converts normalized predictions back to original gene expression scale.\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(normalized_predictions):\n",
    "            normalized_predictions = normalized_predictions.cpu().numpy()\n",
    "\n",
    "        log_predictions = (\n",
    "            normalized_predictions * self.target_log_std\n",
    "        ) + self.target_log_mean\n",
    "        original_scale = np.expm1(log_predictions)\n",
    "        return np.clip(original_scale, 0, None)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.promoter_tensor[idx], self.targets[idx]\n",
    "\n",
    "class PromoterAttentionCNN(nn.Module):\n",
    "    def __init__(self, n_bins: int, n_features: int, n_channels: int):\n",
    "        super().__init__()\n",
    "        in_channels = n_features * n_channels\n",
    "\n",
    "        # Conv 1\n",
    "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size=7, padding=\"same\")\n",
    "        self.norm1 = nn.GroupNorm(8, 64)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "\n",
    "        # Residual block 1\n",
    "        self.res1_conv1 = nn.Conv1d(64, 64, kernel_size=5, padding=\"same\")\n",
    "        self.res1_norm1 = nn.GroupNorm(8, 64)\n",
    "        self.res1_conv2 = nn.Conv1d(64, 64, kernel_size=5, padding=\"same\")\n",
    "        self.res1_norm2 = nn.GroupNorm(8, 64)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "\n",
    "        # Conv 2\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=\"same\")\n",
    "        self.norm2 = nn.GroupNorm(8, 128)\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        \n",
    "        # Residual block 2\n",
    "        self.res2_conv1 = nn.Conv1d(128, 128, kernel_size=3, padding=\"same\")\n",
    "        self.res2_norm1 = nn.GroupNorm(8, 128)\n",
    "        self.res2_conv2 = nn.Conv1d(128, 128, kernel_size=3, padding=\"same\")\n",
    "        self.res2_norm2 = nn.GroupNorm(8, 128)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "\n",
    "        # Additional conv layer\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=\"same\")\n",
    "        self.norm3 = nn.GroupNorm(8, 256)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool1d(32)\n",
    "\n",
    "        # Multi-head attention\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            embed_dim=256, num_heads=8, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.attn_norm = nn.LayerNorm(256)\n",
    "\n",
    "        # Prediction head\n",
    "        self.fc1 = nn.Linear(256 * 32, 512)\n",
    "        self.fc_norm = nn.LayerNorm(512)\n",
    "        self.fc_dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc_dropout2 = nn.Dropout(0.1)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def _residual_block(self, x, conv1, norm1, conv2, norm2):\n",
    "        \"\"\"Residual block with skip connection.\"\"\"\n",
    "        identity = x\n",
    "        out = F.relu(norm1(conv1(x)))\n",
    "        out = norm2(conv2(out))\n",
    "        out += identity  # Skip connection\n",
    "        return F.relu(out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial conv\n",
    "        x = F.relu(self.norm1(self.conv1(x)))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Residual block 1\n",
    "        x = self._residual_block(\n",
    "            x, self.res1_conv1, self.res1_norm1, self.res1_conv2, self.res1_norm2\n",
    "        )\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Conv 2\n",
    "        x = F.relu(self.norm2(self.conv2(x)))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Residual block 2\n",
    "        x = self._residual_block(\n",
    "            x, self.res2_conv1, self.res2_norm1, self.res2_conv2, self.res2_norm2\n",
    "        )\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Conv 3\n",
    "        x = F.relu(self.norm3(self.conv3(x)))\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # GAP\n",
    "        x = self.gap(x)  # (batch, 256, 32)\n",
    "\n",
    "        # Self-attention\n",
    "        x_t = x.transpose(1, 2)\n",
    "        attn_out, _ = self.self_attn(x_t, x_t, x_t)\n",
    "        x_t = self.attn_norm(x_t + attn_out)  # Residual\n",
    "\n",
    "        # Flatten and predict\n",
    "        h = torch.flatten(x_t, 1)\n",
    "        h = F.relu(self.fc_norm(self.fc1(h)))\n",
    "        h = self.fc_dropout(h)\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = self.fc_dropout2(h)\n",
    "        return self.fc3(h).squeeze(1)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_epoch_scorer(net, dataset_valid, y=None):\n",
    "    \"\"\"\n",
    "    Custom skorch scorer to calculate Spearman correlation on the validation set.\n",
    "\n",
    "    This function correctly handles skorch's behavior by ignoring the potentially\n",
    "    incomplete 'y' parameter and reconstructing the full y_true array from the\n",
    "    provided validation dataset.\n",
    "    \"\"\"\n",
    "    y_pred = net.predict(dataset_valid).ravel()\n",
    "    y_true = np.array([y_i.item() for _, y_i in dataset_valid]).ravel()\n",
    "\n",
    "    correlation, _ = spearmanr(y_true, y_pred)\n",
    "\n",
    "    # This is a necessary sanity check, not error hiding. spearmanr can return\n",
    "    # NaN if all predictions are identical\n",
    "    if np.isnan(correlation):\n",
    "        return 0.0\n",
    "\n",
    "    return float(correlation)\n",
    "\n",
    "\n",
    "def validate_dataset(dataset):\n",
    "    \"\"\"Validates dataset for NaN, Inf, and prints value ranges.\"\"\"\n",
    "    print(\"\\n=== Data Validation ===\")\n",
    "\n",
    "    has_nan = torch.isnan(dataset.promoter_tensor).any()\n",
    "    has_inf = torch.isinf(dataset.promoter_tensor).any()\n",
    "    print(f\"Promoter - NaN: {has_nan}, Inf: {has_inf}\")\n",
    "    print(\n",
    "        f\"Promoter range: [{dataset.promoter_tensor.min():.3f}, {dataset.promoter_tensor.max():.3f}]\"\n",
    "    )\n",
    "\n",
    "    has_nan = torch.isnan(dataset.targets).any()\n",
    "    has_inf = torch.isinf(dataset.targets).any()\n",
    "    print(f\"Targets - NaN: {has_nan}, Inf: {has_inf}\")\n",
    "    print(f\"Targets range: [{dataset.targets.min():.3f}, {dataset.targets.max():.3f}]\")\n",
    "    print(\n",
    "        f\"Targets with value 0.0: {(dataset.targets == 0.0).sum()} / {len(dataset.targets)}\"\n",
    "    )\n",
    "    print(\"=====================\\n\")\n",
    "\n",
    "\n",
    "def train_with_skorch(\n",
    "    full_ds: Dataset,\n",
    "    model: nn.Module,\n",
    "    *,\n",
    "    device: torch.device,\n",
    "    batch_size: int = 64,\n",
    "    max_epochs: int = 10_000,\n",
    "    learning_rate: float = 1e-3,\n",
    "    num_workers: int = 0,\n",
    "    patience: int = 10,\n",
    "    min_delta: float = 1e-4,\n",
    "    checkpoint_dir: str = \"checkpoints\",\n",
    "    monitor_name: str = \"valid_spearman\",\n",
    "):\n",
    "    N = len(full_ds)\n",
    "    indices = list(range(N))\n",
    "    train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_ds = Subset(full_ds, train_idx)\n",
    "    valid_ds = Subset(full_ds, val_idx)\n",
    "\n",
    "    pin_memory = device.type == \"cuda\"\n",
    "    callbacks = [\n",
    "        EpochScoring(\n",
    "            spearman_epoch_scorer,\n",
    "            lower_is_better=False,\n",
    "            name=monitor_name,\n",
    "            use_caching=False,\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=monitor_name,\n",
    "            patience=patience,\n",
    "            threshold=min_delta,\n",
    "            lower_is_better=False,\n",
    "        ),\n",
    "        Checkpoint(\n",
    "            dirname=checkpoint_dir,\n",
    "            monitor=f\"{monitor_name}_best\",\n",
    "            f_params=\"best_model.pt\",\n",
    "        ),\n",
    "        LRScheduler(\n",
    "            policy=CosineAnnealingWarmRestarts,\n",
    "            T_0=10,\n",
    "            T_mult=2,\n",
    "            eta_min=1e-6,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    class PearsonCorrelationLoss(nn.Module):\n",
    "        \"\"\"Differentiable Pearson correlation loss (1 - correlation)\"\"\"\n",
    "\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "        def forward(self, y_pred, y_true):\n",
    "            y_pred = y_pred.view(-1)\n",
    "            y_true = y_true.view(-1)\n",
    "\n",
    "            vx = y_pred - torch.mean(y_pred)\n",
    "            vy = y_true - torch.mean(y_true)\n",
    "\n",
    "            cost = torch.sum(vx * vy) / (\n",
    "                torch.sqrt(torch.sum(vx**2)) * torch.sqrt(torch.sum(vy**2)) + 1e-8\n",
    "            )\n",
    "            return 1 - cost\n",
    "\n",
    "    net = NeuralNetRegressor(\n",
    "        model,\n",
    "        criterion=PearsonCorrelationLoss,\n",
    "        optimizer=optim.AdamW,\n",
    "        optimizer__lr=learning_rate,\n",
    "        optimizer__weight_decay=1e-4,\n",
    "        max_epochs=max_epochs,\n",
    "        batch_size=batch_size,\n",
    "        device=device.type,\n",
    "        train_split=predefined_split(valid_ds),\n",
    "        callbacks=callbacks,\n",
    "        iterator_train__num_workers=num_workers,\n",
    "        iterator_valid__num_workers=num_workers,\n",
    "        iterator_train__pin_memory=pin_memory,\n",
    "        iterator_valid__pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    print(f\"Combined dataset size: {N}\")\n",
    "    print(f\"Training set size:   {len(train_idx)}\")\n",
    "    print(f\"Validation set size: {len(val_idx)}\")\n",
    "\n",
    "    net.fit(train_ds, y=None)\n",
    "\n",
    "    from pathlib import Path\n",
    "\n",
    "    net.load_params(f_params=Path(checkpoint_dir) / \"best_model.pt\")\n",
    "    best_torch_model = net.module_\n",
    "    return net, best_torch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Creating dataset...\n",
      "\n",
      "============================================================\n",
      "TRAINING MODE: Computing per-feature normalization\n",
      "============================================================\n",
      "Computed stats for 7 features (1 channel: mean)\n",
      "  Mean stats tensor shape: torch.Size([7])\n",
      "  Std stats tensor shape:  torch.Size([7])\n",
      "\n",
      "Target normalization:\n",
      "  log1p mean: 1.256, std: 1.925\n",
      "\n",
      "Dataset size: 32568 samples\n",
      "Tensor shape: (32568, 7, 1536)\n",
      "============================================================\n",
      "\n",
      "\n",
      "=== Data Validation ===\n",
      "Promoter - NaN: False, Inf: False\n",
      "Promoter range: [-1.020, 36.535]\n",
      "Targets - NaN: False, Inf: False\n",
      "Targets range: [-0.653, 4.481]\n",
      "Targets with value 0.0: 0 / 32568\n",
      "=====================\n",
      "\n",
      "Starting training...\n",
      "Combined dataset size: 32568\n",
      "Training set size:   26054\n",
      "Validation set size: 6514\n",
      "  epoch    train_loss    valid_loss    valid_spearman    cp      lr       dur\n",
      "-------  ------------  ------------  ----------------  ----  ------  --------\n",
      "      1        \u001b[36m0.3352\u001b[0m        \u001b[32m0.2379\u001b[0m            \u001b[35m0.7509\u001b[0m     +  0.0003  457.2414\n",
      "\n",
      "Saved best model to: /workspaces/Gene-Expression-Prediction/data/best_model/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_WORKERS = 2\n",
    "MAX_EPOCHS = 100\n",
    "PATIENCE = 15\n",
    "MIN_DELTA = 1e-4\n",
    "MODEL_SAVE_PATH = (\n",
    "    \"/workspaces/Gene-Expression-Prediction/data/best_model/best_model.pth\"\n",
    ")\n",
    "\n",
    "print(\"Creating dataset...\")\n",
    "full_ds = GeneExpressionDataset(combined_features)\n",
    "validate_dataset(full_ds)\n",
    "\n",
    "model = PromoterAttentionCNN(\n",
    "    n_bins=combined_features.n_bins,\n",
    "    n_features=7,\n",
    "    n_channels=1,\n",
    ")\n",
    "\n",
    "# Run Training\n",
    "print(\"Starting training...\")\n",
    "net, best_model = train_with_skorch(\n",
    "    full_ds=full_ds,\n",
    "    model=model,\n",
    "    device=DEVICE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    patience=PATIENCE,\n",
    "    min_delta=MIN_DELTA,\n",
    "    checkpoint_dir=\"checkpoints_promoter_only\",\n",
    ")\n",
    "\n",
    "torch.save(best_model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"\\nSaved best model to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.3 - Prediction on Test Data (Evaluation Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting on 1984 test genes\n",
      "\n",
      "Step 1: Loading training data and computing normalization...\n",
      "\n",
      "============================================================\n",
      "TRAINING MODE: Computing normalization parameters\n",
      "============================================================\n",
      "\n",
      "Promoter normalization (global per channel):\n",
      "  Mean channel: μ=1.015, σ=5.113\n",
      "  Max channel:  μ=2.123, σ=8.314\n",
      "\n",
      "Target normalization:\n",
      "  Original range: [0.0, 19519.8]\n",
      "  Log range: [0.000, 9.879]\n",
      "  Normalized range: [-0.653, 4.481]\n",
      "\n",
      "Dataset size: 32568 samples\n",
      "Tensor shape: (32568, 14, 190)\n",
      "============================================================\n",
      "\n",
      "\n",
      "Step 2: Creating test dataset with training normalization...\n",
      "\n",
      "============================================================\n",
      "TEST MODE: Applying training normalization parameters\n",
      "============================================================\n",
      "\n",
      "Using training normalization stats:\n",
      "  Mean channel: μ=1.015, std=5.113\n",
      "  Max channel:  μ=2.123, std=8.314\n",
      "\n",
      "Dataset size: 1984 samples\n",
      "Tensor shape: (1984, 14, 190)\n",
      "============================================================\n",
      "\n",
      "\n",
      "Step 3: Loading trained model...\n",
      "\n",
      "Step 4: Making predictions...\n",
      "\n",
      "Step 5: Denormalizing predictions...\n",
      "\n",
      "============================================================\n",
      "PREDICTION SUMMARY\n",
      "============================================================\n",
      "  Shape: (1984,)\n",
      "  Range: [0.000, 172654288.607]\n",
      "  Mean:  273655.766\n",
      "  Median: 0.000\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = None\n",
    "test_genes = cell_line_x3.gene_annotations[\"gene_name\"].values\n",
    "\n",
    "print(f\"\\nPredicting on {len(test_genes)} test genes\\n\")\n",
    "\n",
    "# Create training dataset (computes normalization)\n",
    "print(\"Step 1: Loading training data and computing normalization...\")\n",
    "train_ds = GeneExpressionDataset(combined_features)\n",
    "\n",
    "# Get normalization params\n",
    "norm_params = train_ds.get_normalization_params()\n",
    "\n",
    "# Create test dataset (applies training normalization)\n",
    "print(\"\\nStep 2: Creating test dataset with training normalization...\")\n",
    "test_ds = GeneExpressionDataset(cell_line_x3, normalize_params=norm_params)\n",
    "\n",
    "# Create and load model\n",
    "print(\"\\nStep 3: Loading trained model...\")\n",
    "model = PromoterAttentionCNN(\n",
    "    n_bins=combined_features.n_bins,\n",
    "    n_features=7,\n",
    "    n_channels=1,\n",
    ")\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    model,\n",
    "    device=DEVICE.type,\n",
    ")\n",
    "\n",
    "net.initialize()\n",
    "net.load_params(\n",
    "    f_params=\"/workspaces/Gene-Expression-Prediction/src/gene_expression_prediction/checkpoints_promoter_only/best_model.pt\"\n",
    ")\n",
    "\n",
    "# Predict (returns normalized predictions)\n",
    "print(\"\\nStep 4: Making predictions...\")\n",
    "pred_normalized = net.predict(test_ds)\n",
    "\n",
    "# Denormalize back to original scale\n",
    "print(\"\\nStep 5: Denormalizing predictions...\")\n",
    "pred = train_ds.denormalize_targets(pred_normalized)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"PREDICTION SUMMARY\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"  Shape: {pred.shape}\")\n",
    "print(f\"  Range: [{pred.min():.3f}, {pred.max():.3f}]\")\n",
    "print(f\"  Mean:  {pred.mean():.3f}\")\n",
    "print(f\"  Median: {np.median(pred):.3f}\")\n",
    "print(f\"{'=' * 60}\\n\")\n",
    "\n",
    "# Validation\n",
    "assert isinstance(pred, np.ndarray), \"Prediction must be numpy array\"\n",
    "assert np.issubdtype(pred.dtype, np.number), \"Prediction must be numeric\"\n",
    "assert pred.shape[0] == len(test_genes), \"One prediction per gene\"\n",
    "assert not np.isnan(pred).any(), \"No NaN values\"\n",
    "assert not np.isinf(pred).any(), \"No Inf values\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Predictions in the Required Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: /workspaces/Gene-Expression-Prediction/data/output/Tokar_David_Project1.zip\n",
      "\n",
      "Preview of the first 5 rows of the submission file:\n",
      "    gene_name  gex_predicted\n",
      "0       CAPN9            0.0\n",
      "1        ILF2            0.0\n",
      "2  ST6GALNAC5            0.0\n",
      "3  MROH7-TTC4            0.0\n",
      "4        AGO4            0.0\n"
     ]
    }
   ],
   "source": [
    "# Store predictions in a ZIP.\n",
    "# Upload this zip on the project website under \"Your submission\".\n",
    "# Zip this notebook along with the conda environment (and README, optional) and upload this under \"Your code\".\n",
    "save_dir = \"/workspaces/Gene-Expression-Prediction/data/output\"\n",
    "file_name = \"gex_predicted.csv\"  # PLEASE DO NOT CHANGE THIS\n",
    "zip_name = \"Tokar_David_Project1.zip\"\n",
    "save_path = f\"{save_dir}/{zip_name}\"\n",
    "compression_options = {\"method\": \"zip\", \"archive_name\": file_name}\n",
    "\n",
    "submission_df = pd.DataFrame({\"gene_name\": test_genes, \"gex_predicted\": pred})\n",
    "\n",
    "compression_options = {\"method\": \"zip\", \"archive_name\": file_name}\n",
    "\n",
    "submission_df.to_csv(save_path, index=False, compression=compression_options)\n",
    "print(f\"File saved to: {save_path}\")\n",
    "print(\"\\nPreview of the first 5 rows of the submission file:\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
