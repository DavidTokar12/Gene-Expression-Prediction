{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skorch import NeuralNetRegressor\n",
    "from skorch.callbacks import Checkpoint\n",
    "from skorch.callbacks import EarlyStopping\n",
    "from skorch.callbacks import EpochScoring\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from gene_expression_prediction.data_loader import BigWigReader\n",
    "from gene_expression_prediction.data_loader import GeneReader\n",
    "from gene_expression_prediction.data_processor import CellLine\n",
    "from gene_expression_prediction.data_processor import FeatureNames\n",
    "from gene_expression_prediction.data_processor import ProcessedFeatures\n",
    "from gene_expression_prediction.data_processor import load_processed_features\n",
    "from gene_expression_prediction.data_processor import process_cell_line\n",
    "from gene_expression_prediction.data_processor import save_processed_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.1 - Modeling Choices & Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required data paths have been successfully validated.\n"
     ]
    }
   ],
   "source": [
    "# TODO:\n",
    "# Load your feature (bed and/or bigwig and/or fasta) and target files (tsv) here.\n",
    "# Decide which features to use for training. Feel free to process them however you need.\n",
    "\n",
    "# NOTE:\n",
    "# bed and bigwig files contain signals of all chromosomes (including sex chromosomes).\n",
    "# Training and validation split based on chromosomes has been done for you.\n",
    "# However, you can resplit the data in any way you want.\n",
    "\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "data_path = \"/workspaces/Gene-Expression-Prediction/data\"\n",
    "info_data = GeneReader(data_path)\n",
    "bigwig_data = BigWigReader(data_path)\n",
    "\n",
    "PROMOTER_WINDOW_SIZE = 5000\n",
    "PROMOTER_BIN_SIZE = 100\n",
    "\n",
    "DISTAL_WINDOW_SIZE = 50_000\n",
    "DISTAL_WINDOW_SIZE_BIN_SIZE=1000\n",
    "SAMPLE_N = None\n",
    "\n",
    "# Takes around 30 minutes to run\n",
    "# cell_line_x1 = process_cell_line(\n",
    "#     cell_line=CellLine.X1,\n",
    "#     gene_reader=info_data,\n",
    "#     bigwig_reader=bigwig_data,\n",
    "#     promoter_window_size=PROMOTER_WINDOW_SIZE,\n",
    "#     distal_window_size=DISTAL_WINDOW_SIZE,\n",
    "#     promoter_bin_size=PROMOTER_BIN_SIZE,\n",
    "#     distal_bin_size=DISTAL_WINDOW_SIZE_BIN_SIZE,\n",
    "#     sample_n=SAMPLE_N,\n",
    "# )\n",
    "# save_processed_features(\n",
    "#     cell_line_x1, \"/workspaces/Gene-Expression-Prediction/data/processed_data_x1\"\n",
    "# )\n",
    "cell_line_x1 = load_processed_features(\n",
    "    \"/workspaces/Gene-Expression-Prediction/data/processed_data_x1\"\n",
    ")\n",
    "\n",
    "\n",
    "# cell_line_x2 = process_cell_line(\n",
    "#     cell_line=CellLine.X2,\n",
    "#     gene_reader=info_data,\n",
    "#     bigwig_reader=bigwig_data,\n",
    "#     promoter_window_size=PROMOTER_WINDOW_SIZE,\n",
    "#     distal_window_size=DISTAL_WINDOW_SIZE,\n",
    "#     promoter_bin_size=PROMOTER_BIN_SIZE,\n",
    "#     distal_bin_size=DISTAL_WINDOW_SIZE_BIN_SIZE,\n",
    "#     sample_n=SAMPLE_N,\n",
    "# )\n",
    "# save_processed_features(\n",
    "#     cell_line_x2, \"/workspaces/Gene-Expression-Prediction/data/processed_data_x2\"\n",
    "# )\n",
    "cell_line_x2 = load_processed_features(\n",
    "    \"/workspaces/Gene-Expression-Prediction/data/processed_data_x2\"\n",
    ")\n",
    "\n",
    "\n",
    "# cell_line_x3 = process_cell_line(\n",
    "#     cell_line=CellLine.X3,\n",
    "#     gene_reader=info_data,\n",
    "#     bigwig_reader=bigwig_data,\n",
    "#     promoter_window_size=PROMOTER_WINDOW_SIZE,\n",
    "#     distal_window_size=DISTAL_WINDOW_SIZE,\n",
    "#     promoter_bin_size=PROMOTER_BIN_SIZE,\n",
    "#     distal_bin_size=DISTAL_WINDOW_SIZE_BIN_SIZE,\n",
    "#     sample_n=None,\n",
    "# )\n",
    "# save_processed_features(\n",
    "#     cell_line_x3, \"/workspaces/Gene-Expression-Prediction/data/processed_data_x3\"\n",
    "# )\n",
    "\n",
    "cell_line_x3 = load_processed_features(\n",
    "    \"/workspaces/Gene-Expression-Prediction/data/processed_data_x3\"\n",
    ")\n",
    "# ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining cell line features...\n",
      "\n",
      "============================================================\n",
      "VALIDATING COMBINED FEATURES ALIGNMENT\n",
      "============================================================\n",
      "Number of genes: 32568\n",
      "Promoter tensor shape: (32568, 190, 7, 2)\n",
      "Gene annotations shape: (32568, 7)\n",
      "Target expression shape: (32568,)\n",
      "All alignments validated successfully!\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def validate_combined_features(features: ProcessedFeatures):\n",
    "    \"\"\"Validates that all components of ProcessedFeatures are properly aligned.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"VALIDATING COMBINED FEATURES ALIGNMENT\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    n_genes = len(features.gene_annotations)\n",
    "    gene_names = features.gene_annotations[\"gene_name\"].values\n",
    "\n",
    "    print(f\"Number of genes: {n_genes}\")\n",
    "    print(f\"Promoter tensor shape: {features.sequence_signal_tensor.shape}\")\n",
    "    print(f\"Gene annotations shape: {features.gene_annotations.shape}\")\n",
    "    print(\n",
    "        f\"Target expression shape: {features.target_expression.shape if features.target_expression is not None else 'None'}\"\n",
    "    )\n",
    "\n",
    "    if features.sequence_signal_tensor.shape[0] != n_genes:\n",
    "        print(\"ERROR: Promoter tensor length doesn't match gene annotations!\")\n",
    "        return False\n",
    "\n",
    "    if features.target_expression is not None:\n",
    "        if len(features.target_expression) != n_genes:\n",
    "            print(\"ERROR: Target expression length doesn't match gene annotations!\")\n",
    "            return False\n",
    "\n",
    "        target_genes = features.target_expression.index.values\n",
    "        if not np.array_equal(gene_names, target_genes):\n",
    "            print(\n",
    "                \"ERROR: Target expression index doesn't match gene_annotations order!\"\n",
    "            )\n",
    "            print(f\"  First 5 in annotations: {gene_names[:5]}\")\n",
    "            print(f\"  First 5 in targets: {target_genes[:5]}\")\n",
    "            return False\n",
    "\n",
    "    print(\"All alignments validated successfully!\")\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def combine_cell_lines(\n",
    "    cell_line_x1: ProcessedFeatures, cell_line_x2: ProcessedFeatures\n",
    ") -> ProcessedFeatures:\n",
    "    \"\"\"\n",
    "    Combines two ProcessedFeatures objects ensuring proper alignment.\n",
    "    \"\"\"\n",
    "    print(\"\\nCombining cell line features...\")\n",
    "\n",
    "    combined_tensor = np.concatenate(\n",
    "        [cell_line_x1.sequence_signal_tensor, cell_line_x2.sequence_signal_tensor],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    annotations_x1 = cell_line_x1.gene_annotations.copy()\n",
    "    annotations_x2 = cell_line_x2.gene_annotations.copy()\n",
    "    annotations_x1[\"gene_name\"] = annotations_x1[\"gene_name\"] + \"_x1\"\n",
    "    annotations_x2[\"gene_name\"] = annotations_x2[\"gene_name\"] + \"_x2\"\n",
    "    combined_annotations = pd.concat(\n",
    "        [annotations_x1, annotations_x2], ignore_index=True\n",
    "    )\n",
    "\n",
    "    targets_x1 = cell_line_x1.target_expression.copy()\n",
    "    targets_x2 = cell_line_x2.target_expression.copy()\n",
    "    targets_x1.index = targets_x1.index + \"_x1\"\n",
    "    targets_x2.index = targets_x2.index + \"_x2\"\n",
    "    combined_targets = pd.concat([targets_x1, targets_x2])\n",
    "\n",
    "    combined_targets = combined_targets.reindex(combined_annotations[\"gene_name\"])\n",
    "\n",
    "    combined_features = ProcessedFeatures(\n",
    "        gene_annotations=combined_annotations,\n",
    "        sequence_signal_tensor=combined_tensor,\n",
    "        target_expression=combined_targets,\n",
    "        n_upstream_bins=cell_line_x1.n_upstream_bins,\n",
    "        n_promoter_bins=cell_line_x1.n_promoter_bins,\n",
    "        n_downstream_bins=cell_line_x1.n_downstream_bins,\n",
    "        n_total_bins=cell_line_x1.n_total_bins,\n",
    "    )\n",
    "\n",
    "    if not validate_combined_features(combined_features):\n",
    "        raise ValueError(\"Combined features validation failed! Data is misaligned.\")\n",
    "\n",
    "    return combined_features\n",
    "\n",
    "\n",
    "combined_features = combine_cell_lines(cell_line_x1, cell_line_x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.2 - Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Select the best model to predict gene expression from the obtained features in WP 1.1.\n",
    "# pytorch_dataset.py\n",
    "class GeneExpressionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for gene expression prediction.\n",
    "\n",
    "    Handles both training (computes normalization) and test (applies training normalization).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, processed_features: ProcessedFeatures, normalize_params=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            processed_features: ProcessedFeatures object\n",
    "            normalize_params: Optional dict with normalization params from training.\n",
    "                            If None, computes new params (training mode).\n",
    "                            If provided, uses these params (test mode).\n",
    "        \"\"\"\n",
    "        is_training = normalize_params is None\n",
    "        requires_targets = processed_features.target_expression is not None\n",
    "\n",
    "        if is_training and not requires_targets:\n",
    "            raise ValueError(\"Target expression required for training dataset.\")\n",
    "\n",
    "        # Process sequence tensor\n",
    "        pt = torch.from_numpy(processed_features.sequence_signal_tensor).float()\n",
    "        pt = pt.permute(0, 2, 3, 1).contiguous()  # (N, F, C, B)\n",
    "        N, F, C, B = pt.shape\n",
    "        self.promoter_tensor = pt.view(N, F * C, B)\n",
    "\n",
    "        # Identify channel indices (interleaved: mean, max, mean, max, ...)\n",
    "        mean_channels = list(range(0, F * C, C))\n",
    "        max_channels = list(range(1, F * C, C))\n",
    "\n",
    "        if is_training:\n",
    "            # TRAINING MODE: Compute normalization parameters\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"TRAINING MODE: Computing normalization parameters\")\n",
    "            print(\"=\" * 60)\n",
    "\n",
    "            mean_data = self.promoter_tensor[:, mean_channels, :]\n",
    "            max_data = self.promoter_tensor[:, max_channels, :]\n",
    "\n",
    "            self.promoter_mean_mean = mean_data.mean().item()\n",
    "            self.promoter_mean_std = mean_data.std().item()\n",
    "            self.promoter_max_mean = max_data.mean().item()\n",
    "            self.promoter_max_std = max_data.std().item()\n",
    "\n",
    "            print(\"\\nPromoter normalization (global per channel):\")\n",
    "            print(\n",
    "                f\"  Mean channel: μ={self.promoter_mean_mean:.3f}, σ={self.promoter_mean_std:.3f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  Max channel:  μ={self.promoter_max_mean:.3f}, σ={self.promoter_max_std:.3f}\"\n",
    "            )\n",
    "\n",
    "            # Normalize targets\n",
    "            target_values = processed_features.target_expression.fillna(0.0).values\n",
    "            log_targets = np.log1p(target_values)\n",
    "\n",
    "            self.target_log_mean = log_targets.mean()\n",
    "            self.target_log_std = log_targets.std()\n",
    "\n",
    "            normalized_targets = (\n",
    "                log_targets - self.target_log_mean\n",
    "            ) / self.target_log_std\n",
    "            self.targets = torch.from_numpy(normalized_targets).float()\n",
    "\n",
    "            print(\"\\nTarget normalization:\")\n",
    "            print(\n",
    "                f\"  Original range: [{target_values.min():.1f}, {target_values.max():.1f}]\"\n",
    "            )\n",
    "            print(f\"  Log range: [{log_targets.min():.3f}, {log_targets.max():.3f}]\")\n",
    "            print(\n",
    "                f\"  Normalized range: [{self.targets.min():.3f}, {self.targets.max():.3f}]\"\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # TEST MODE: Use provided normalization parameters\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"TEST MODE: Applying training normalization parameters\")\n",
    "            print(\"=\" * 60)\n",
    "\n",
    "            self.promoter_mean_mean = normalize_params[\"promoter_mean_mean\"]\n",
    "            self.promoter_mean_std = normalize_params[\"promoter_mean_std\"]\n",
    "            self.promoter_max_mean = normalize_params[\"promoter_max_mean\"]\n",
    "            self.promoter_max_std = normalize_params[\"promoter_max_std\"]\n",
    "            self.target_log_mean = normalize_params[\"target_log_mean\"]\n",
    "            self.target_log_std = normalize_params[\"target_log_std\"]\n",
    "\n",
    "            print(\"\\nUsing training normalization stats:\")\n",
    "            print(\n",
    "                f\"  Mean channel: μ={self.promoter_mean_mean:.3f}, std={self.promoter_mean_std:.3f}\"\n",
    "            )\n",
    "            print(\n",
    "                f\"  Max channel:  μ={self.promoter_max_mean:.3f}, std={self.promoter_max_std:.3f}\"\n",
    "            )\n",
    "\n",
    "            self.targets = torch.zeros(N)\n",
    "\n",
    "        for i in range(F):\n",
    "            # Normalize mean channels\n",
    "            if self.promoter_mean_std > 1e-8:\n",
    "                self.promoter_tensor[:, i * C + 0, :] = (\n",
    "                    self.promoter_tensor[:, i * C + 0, :] - self.promoter_mean_mean\n",
    "                ) / self.promoter_mean_std\n",
    "            else:\n",
    "                self.promoter_tensor[:, i * C + 0, :] = (\n",
    "                    self.promoter_tensor[:, i * C + 0, :] - self.promoter_mean_mean\n",
    "                )\n",
    "\n",
    "            # Normalize max channels\n",
    "            if self.promoter_max_std > 1e-8:\n",
    "                self.promoter_tensor[:, i * C + 1, :] = (\n",
    "                    self.promoter_tensor[:, i * C + 1, :] - self.promoter_max_mean\n",
    "                ) / self.promoter_max_std\n",
    "            else:\n",
    "                self.promoter_tensor[:, i * C + 1, :] = (\n",
    "                    self.promoter_tensor[:, i * C + 1, :] - self.promoter_max_mean\n",
    "                )\n",
    "\n",
    "        print(f\"\\nDataset size: {N} samples\")\n",
    "        print(f\"Tensor shape: {tuple(self.promoter_tensor.shape)}\")\n",
    "        print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    def get_normalization_params(self):\n",
    "        \"\"\"\n",
    "        Returns normalization parameters for use with test set.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing all normalization parameters\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"promoter_mean_mean\": self.promoter_mean_mean,\n",
    "            \"promoter_mean_std\": self.promoter_mean_std,\n",
    "            \"promoter_max_mean\": self.promoter_max_mean,\n",
    "            \"promoter_max_std\": self.promoter_max_std,\n",
    "            \"target_log_mean\": self.target_log_mean,\n",
    "            \"target_log_std\": self.target_log_std,\n",
    "        }\n",
    "\n",
    "    def denormalize_targets(self, normalized_predictions):\n",
    "        \"\"\"\n",
    "        Converts normalized predictions back to original gene expression scale.\n",
    "\n",
    "        Args:\n",
    "            normalized_predictions: Normalized predictions (numpy array or tensor)\n",
    "\n",
    "        Returns:\n",
    "            numpy array: Predictions in original scale\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(normalized_predictions):\n",
    "            normalized_predictions = normalized_predictions.cpu().numpy()\n",
    "\n",
    "        log_predictions = (\n",
    "            normalized_predictions * self.target_log_std\n",
    "        ) + self.target_log_mean\n",
    "        original_scale = np.expm1(log_predictions)\n",
    "        return np.clip(original_scale, 0, None)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        return self.promoter_tensor[idx], self.targets[idx]\n",
    "\n",
    "\n",
    "class PromoterAttentionCNN(nn.Module):\n",
    "    def __init__(self, n_bins: int, n_features: int, n_channels: int):\n",
    "        super().__init__()\n",
    "        in_channels = n_features * n_channels\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size=7, padding=\"same\")\n",
    "        self.norm1 = nn.GroupNorm(8, 64)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        # Residual block 1\n",
    "        self.res1_conv1 = nn.Conv1d(64, 64, kernel_size=5, padding=\"same\")\n",
    "        self.res1_norm1 = nn.GroupNorm(8, 64)\n",
    "        self.res1_conv2 = nn.Conv1d(64, 64, kernel_size=5, padding=\"same\")\n",
    "        self.res1_norm2 = nn.GroupNorm(8, 64)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=5, padding=\"same\")\n",
    "        self.norm2 = nn.GroupNorm(8, 128)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        # Residual block 2\n",
    "        self.res2_conv1 = nn.Conv1d(128, 128, kernel_size=3, padding=\"same\")\n",
    "        self.res2_norm1 = nn.GroupNorm(8, 128)\n",
    "        self.res2_conv2 = nn.Conv1d(128, 128, kernel_size=3, padding=\"same\")\n",
    "        self.res2_norm2 = nn.GroupNorm(8, 128)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "\n",
    "        # Additional conv layer\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=3, padding=\"same\")\n",
    "        self.norm3 = nn.GroupNorm(8, 256)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "        self.gap = nn.AdaptiveAvgPool1d(32)\n",
    "\n",
    "        # Multi-head attention\n",
    "        self.self_attn = nn.MultiheadAttention(\n",
    "            embed_dim=256, num_heads=8, dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.attn_norm = nn.LayerNorm(256)\n",
    "\n",
    "        # Prediction head\n",
    "        self.fc1 = nn.Linear(256 * 32, 512)\n",
    "        self.fc_norm = nn.LayerNorm(512)\n",
    "        self.fc_dropout = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc_dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "\n",
    "    def _residual_block(self, x, conv1, norm1, conv2, norm2):\n",
    "        \"\"\"Residual block with skip connection.\"\"\"\n",
    "        identity = x\n",
    "        out = F.relu(norm1(conv1(x)))\n",
    "        out = norm2(conv2(out))\n",
    "        out += identity  # Skip connection\n",
    "        return F.relu(out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial conv\n",
    "        x = F.relu(self.norm1(self.conv1(x)))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Residual block 1\n",
    "        x = self._residual_block(\n",
    "            x, self.res1_conv1, self.res1_norm1, self.res1_conv2, self.res1_norm2\n",
    "        )\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        # Conv 2\n",
    "        x = F.relu(self.norm2(self.conv2(x)))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Residual block 2\n",
    "        x = self._residual_block(\n",
    "            x, self.res2_conv1, self.res2_norm1, self.res2_conv2, self.res2_norm2\n",
    "        )\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        # Conv 3\n",
    "        x = F.relu(self.norm3(self.conv3(x)))\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # GAP\n",
    "        x = self.gap(x)  # (batch, 256, 32)\n",
    "\n",
    "        # Self-attention\n",
    "        x_t = x.transpose(1, 2)\n",
    "        attn_out, _ = self.self_attn(x_t, x_t, x_t)\n",
    "        x_t = self.attn_norm(x_t + attn_out)  # Residual\n",
    "\n",
    "        # Flatten and predict\n",
    "        h = torch.flatten(x_t, 1)\n",
    "        h = F.relu(self.fc_norm(self.fc1(h)))\n",
    "        h = self.fc_dropout(h)\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = self.fc_dropout2(h)\n",
    "        return self.fc3(h).squeeze(1)\n",
    "# ----------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spearman_epoch_scorer(net, dataset_valid, y=None):\n",
    "    \"\"\"\n",
    "    Custom skorch scorer to calculate Spearman correlation on the validation set.\n",
    "\n",
    "    This function correctly handles skorch's behavior by ignoring the potentially\n",
    "    incomplete 'y' parameter and reconstructing the full y_true array from the\n",
    "    provided validation dataset.\n",
    "    \"\"\"\n",
    "    y_pred = net.predict(dataset_valid).ravel()\n",
    "    y_true = np.array([y_i.item() for _, y_i in dataset_valid]).ravel()\n",
    "\n",
    "    correlation, _ = spearmanr(y_true, y_pred)\n",
    "\n",
    "    # This is a necessary sanity check, not error hiding. spearmanr can return\n",
    "    # NaN if all predictions are identical\n",
    "    if np.isnan(correlation):\n",
    "        return 0.0\n",
    "\n",
    "    return float(correlation)\n",
    "\n",
    "\n",
    "def validate_dataset(dataset):\n",
    "    \"\"\"Validates dataset for NaN, Inf, and prints value ranges.\"\"\"\n",
    "    print(\"\\n=== Data Validation ===\")\n",
    "\n",
    "    has_nan = torch.isnan(dataset.promoter_tensor).any()\n",
    "    has_inf = torch.isinf(dataset.promoter_tensor).any()\n",
    "    print(f\"Promoter - NaN: {has_nan}, Inf: {has_inf}\")\n",
    "    print(\n",
    "        f\"Promoter range: [{dataset.promoter_tensor.min():.3f}, {dataset.promoter_tensor.max():.3f}]\"\n",
    "    )\n",
    "\n",
    "    has_nan = torch.isnan(dataset.targets).any()\n",
    "    has_inf = torch.isinf(dataset.targets).any()\n",
    "    print(f\"Targets - NaN: {has_nan}, Inf: {has_inf}\")\n",
    "    print(f\"Targets range: [{dataset.targets.min():.3f}, {dataset.targets.max():.3f}]\")\n",
    "    print(\n",
    "        f\"Targets with value 0.0: {(dataset.targets == 0.0).sum()} / {len(dataset.targets)}\"\n",
    "    )\n",
    "    print(\"=====================\\n\")\n",
    "\n",
    "\n",
    "def train_with_skorch(\n",
    "    full_ds: Dataset,\n",
    "    model: nn.Module,\n",
    "    *,\n",
    "    device: torch.device,\n",
    "    batch_size: int = 64,\n",
    "    max_epochs: int = 10_000,\n",
    "    learning_rate: float = 1e-3,\n",
    "    num_workers: int = 0,\n",
    "    patience: int = 10,\n",
    "    min_delta: float = 1e-4,\n",
    "    checkpoint_dir: str = \"checkpoints\",\n",
    "    monitor_name: str = \"valid_spearman\",\n",
    "):\n",
    "    N = len(full_ds)\n",
    "    indices = list(range(N))\n",
    "    train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_ds = Subset(full_ds, train_idx)\n",
    "    valid_ds = Subset(full_ds, val_idx)\n",
    "\n",
    "    pin_memory = device.type == \"cuda\"\n",
    "    callbacks = [\n",
    "        EpochScoring(\n",
    "            spearman_epoch_scorer,\n",
    "            lower_is_better=False,\n",
    "            name=monitor_name,\n",
    "            use_caching=False,\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor=monitor_name,\n",
    "            patience=patience,\n",
    "            threshold=min_delta,\n",
    "            lower_is_better=False,\n",
    "        ),\n",
    "        Checkpoint(\n",
    "            dirname=checkpoint_dir,\n",
    "            monitor=f\"{monitor_name}_best\",\n",
    "            f_params=\"best_model.pt\",\n",
    "        ),\n",
    "        # LRScheduler(\n",
    "        #     policy=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        #     mode=\"max\",\n",
    "        #     factor=0.5,\n",
    "        #     patience=max(2, patience // 2),\n",
    "        #     monitor=monitor_name,\n",
    "        # ),\n",
    "        LRScheduler(\n",
    "            policy=CosineAnnealingWarmRestarts,\n",
    "            T_0=10,\n",
    "            T_mult=2,\n",
    "            eta_min=1e-6,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "\n",
    "    class PearsonCorrelationLoss(nn.Module):\n",
    "        \"\"\"Differentiable Pearson correlation loss (1 - correlation)\"\"\"\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "        \n",
    "        def forward(self, y_pred, y_true):\n",
    "            y_pred = y_pred.view(-1)\n",
    "            y_true = y_true.view(-1)\n",
    "            \n",
    "            vx = y_pred - torch.mean(y_pred)\n",
    "            vy = y_true - torch.mean(y_true)\n",
    "            \n",
    "            cost = torch.sum(vx * vy) / (torch.sqrt(torch.sum(vx ** 2)) * torch.sqrt(torch.sum(vy ** 2)) + 1e-8)\n",
    "            return 1 - cost\n",
    "\n",
    "    net = NeuralNetRegressor(\n",
    "        model,\n",
    "        criterion=PearsonCorrelationLoss,\n",
    "        optimizer=optim.AdamW,\n",
    "        optimizer__lr=learning_rate,\n",
    "        optimizer__weight_decay=1e-4,\n",
    "        max_epochs=max_epochs,\n",
    "        batch_size=batch_size,\n",
    "        device=device.type,\n",
    "        train_split=predefined_split(valid_ds),\n",
    "        callbacks=callbacks,\n",
    "        iterator_train__num_workers=num_workers,\n",
    "        iterator_valid__num_workers=num_workers,\n",
    "        iterator_train__pin_memory=pin_memory,\n",
    "        iterator_valid__pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    print(f\"Combined dataset size: {N}\")\n",
    "    print(f\"Training set size:   {len(train_idx)}\")\n",
    "    print(f\"Validation set size: {len(val_idx)}\")\n",
    "\n",
    "    net.fit(train_ds, y=None)\n",
    "\n",
    "    from pathlib import Path\n",
    "\n",
    "    net.load_params(f_params=Path(checkpoint_dir) / \"best_model.pt\")\n",
    "    best_torch_model = net.module_\n",
    "    return net, best_torch_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Creating dataset...\n",
      "\n",
      "============================================================\n",
      "TRAINING MODE: Computing normalization parameters\n",
      "============================================================\n",
      "\n",
      "Promoter normalization (global per channel):\n",
      "  Mean channel: μ=1.015, σ=5.113\n",
      "  Max channel:  μ=2.123, σ=8.314\n",
      "\n",
      "Target normalization:\n",
      "  Original range: [0.0, 19519.8]\n",
      "  Log range: [0.000, 9.879]\n",
      "  Normalized range: [-0.653, 4.481]\n",
      "\n",
      "Dataset size: 32568 samples\n",
      "Tensor shape: (32568, 14, 190)\n",
      "============================================================\n",
      "\n",
      "\n",
      "=== Data Validation ===\n",
      "Promoter - NaN: False, Inf: False\n",
      "Promoter range: [-0.255, 94.493]\n",
      "Targets - NaN: False, Inf: False\n",
      "Targets range: [-0.653, 4.481]\n",
      "Targets with value 0.0: 0 / 32568\n",
      "=====================\n",
      "\n",
      "Starting training...\n",
      "Combined dataset size: 32568\n",
      "Training set size:   26054\n",
      "Validation set size: 6514\n",
      "  epoch    train_loss    valid_loss    valid_spearman    cp      lr      dur\n",
      "-------  ------------  ------------  ----------------  ----  ------  -------\n",
      "      1        \u001b[36m0.2743\u001b[0m        \u001b[32m0.2353\u001b[0m            \u001b[35m0.7556\u001b[0m     +  0.0003  40.2737\n",
      "      2        \u001b[36m0.2299\u001b[0m        \u001b[32m0.2084\u001b[0m            \u001b[35m0.7653\u001b[0m     +  0.0003  43.4337\n",
      "      3        \u001b[36m0.2027\u001b[0m        \u001b[32m0.1915\u001b[0m            \u001b[35m0.7836\u001b[0m     +  0.0003  41.1661\n",
      "      4        \u001b[36m0.1855\u001b[0m        \u001b[32m0.1793\u001b[0m            \u001b[35m0.7879\u001b[0m     +  0.0002  44.8285\n",
      "      5        \u001b[36m0.1764\u001b[0m        \u001b[32m0.1756\u001b[0m            \u001b[35m0.7920\u001b[0m     +  0.0002  44.9318\n",
      "      6        \u001b[36m0.1682\u001b[0m        \u001b[32m0.1719\u001b[0m            \u001b[35m0.7922\u001b[0m     +  0.0002  43.1129\n",
      "      7        \u001b[36m0.1630\u001b[0m        \u001b[32m0.1668\u001b[0m            \u001b[35m0.7951\u001b[0m     +  0.0001  44.1027\n",
      "      8        \u001b[36m0.1564\u001b[0m        \u001b[32m0.1636\u001b[0m            0.7943        0.0001  43.6342\n",
      "      9        \u001b[36m0.1510\u001b[0m        \u001b[32m0.1625\u001b[0m            0.7938        0.0000  45.7026\n",
      "     10        \u001b[36m0.1476\u001b[0m        \u001b[32m0.1618\u001b[0m            0.7950        0.0000  46.5000\n",
      "     11        0.1647        0.1662            \u001b[35m0.7951\u001b[0m     +  0.0003  45.9731\n",
      "     12        0.1587        0.1620            0.7944        0.0003  46.6414\n",
      "     13        0.1555        0.1641            \u001b[35m0.7962\u001b[0m     +  0.0003  49.5199\n",
      "     14        0.1494        0.1622            0.7958        0.0003  45.6000\n",
      "     15        \u001b[36m0.1456\u001b[0m        0.1639            0.7930        0.0003  45.2102\n",
      "     16        \u001b[36m0.1413\u001b[0m        \u001b[32m0.1612\u001b[0m            0.7940        0.0003  50.5183\n",
      "     17        \u001b[36m0.1379\u001b[0m        0.1626            0.7959        0.0002  46.7313\n",
      "     18        \u001b[36m0.1355\u001b[0m        0.1623            0.7961        0.0002  47.0899\n",
      "     19        \u001b[36m0.1306\u001b[0m        \u001b[32m0.1598\u001b[0m            \u001b[35m0.7970\u001b[0m     +  0.0002  47.2349\n",
      "     20        \u001b[36m0.1259\u001b[0m        0.1610            0.7962        0.0002  51.4066\n",
      "     21        \u001b[36m0.1216\u001b[0m        0.1613            \u001b[35m0.7986\u001b[0m     +  0.0002  48.8313\n",
      "     22        \u001b[36m0.1169\u001b[0m        0.1603            0.7959        0.0001  48.4330\n",
      "     23        \u001b[36m0.1117\u001b[0m        0.1638            0.7975        0.0001  49.2568\n",
      "     24        \u001b[36m0.1090\u001b[0m        0.1620            \u001b[35m0.7987\u001b[0m     +  0.0001  48.1938\n",
      "     25        \u001b[36m0.1062\u001b[0m        0.1655            0.7959        0.0001  50.9284\n",
      "     26        \u001b[36m0.1021\u001b[0m        0.1640            0.7973        0.0000  48.6933\n",
      "     27        \u001b[36m0.0994\u001b[0m        0.1620            0.7979        0.0000  48.2334\n",
      "     28        \u001b[36m0.0978\u001b[0m        0.1616            0.7981        0.0000  46.2399\n",
      "     29        \u001b[36m0.0954\u001b[0m        \u001b[32m0.1596\u001b[0m            \u001b[35m0.7991\u001b[0m     +  0.0000  49.5358\n",
      "     30        0.0954        \u001b[32m0.1591\u001b[0m            \u001b[35m0.7993\u001b[0m     +  0.0000  47.3902\n",
      "     31        0.1170        0.1602            0.7947        0.0003  44.4670\n",
      "     32        0.1155        \u001b[32m0.1585\u001b[0m            0.7974        0.0003  47.1677\n",
      "     33        0.1125        0.1622            0.7941        0.0003  49.9420\n",
      "     34        0.1087        0.1606            0.7982        0.0003  47.6689\n",
      "     35        0.1056        0.1641            0.7985        0.0003  50.9808\n",
      "     36        0.0999        0.1665            0.7936        0.0003  50.1364\n",
      "     37        0.0989        0.1643            0.7965        0.0003  52.3703\n",
      "     38        \u001b[36m0.0937\u001b[0m        0.1652            0.7974        0.0003  54.9918\n",
      "     39        \u001b[36m0.0899\u001b[0m        0.1640            0.7975        0.0003  47.0639\n",
      "     40        \u001b[36m0.0891\u001b[0m        0.1656            0.7941        0.0003  49.6373\n",
      "     41        \u001b[36m0.0843\u001b[0m        0.1677            0.7949        0.0003  51.0781\n",
      "     42        \u001b[36m0.0820\u001b[0m        0.1685            0.7936        0.0002  52.1344\n",
      "     43        \u001b[36m0.0780\u001b[0m        0.1644            0.7965        0.0002  52.6481\n",
      "     44        \u001b[36m0.0745\u001b[0m        0.1623            0.7947        0.0002  55.2102\n",
      "Stopping since valid_spearman has not improved in the last 15 epochs.\n",
      "\n",
      "Saved best model to: ./best_promoter_model.pth\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "PROMOTER_WINDOW_SIZE = 5000\n",
    "PROMOTER_BIN_SIZE = 100\n",
    "N_BINS = (2 * PROMOTER_WINDOW_SIZE) // PROMOTER_BIN_SIZE\n",
    "N_FEATURES = len(FeatureNames)\n",
    "N_CHANNELS = 2 # mean, max\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_WORKERS = 2\n",
    "MAX_EPOCHS = 100\n",
    "PATIENCE = 15\n",
    "MIN_DELTA = 1e-4\n",
    "MODEL_SAVE_PATH = \"/workspaces/Gene-Expression-Prediction/data/best_model/best_model.pth\"\n",
    "\n",
    "print(\"Creating dataset...\")\n",
    "full_ds = GeneExpressionDataset(combined_features)\n",
    "validate_dataset(full_ds)\n",
    "\n",
    "model = PromoterAttentionCNN(\n",
    "    n_bins=N_BINS,\n",
    "    n_features=N_FEATURES,\n",
    "    n_channels=N_CHANNELS,\n",
    ")\n",
    "\n",
    "# Run Training\n",
    "print(\"Starting training...\")\n",
    "net, best_model = train_with_skorch(\n",
    "    full_ds=full_ds,\n",
    "    model=model,\n",
    "    device=DEVICE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    patience=PATIENCE,\n",
    "    min_delta=MIN_DELTA,\n",
    "    checkpoint_dir=\"checkpoints_promoter_only\",\n",
    ")\n",
    "\n",
    "torch.save(best_model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"\\nSaved best model to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.3 - Prediction on Test Data (Evaluation Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicting on 1984 test genes\n",
      "\n",
      "Step 1: Loading training data and computing normalization...\n",
      "\n",
      "============================================================\n",
      "TRAINING MODE: Computing normalization parameters\n",
      "============================================================\n",
      "\n",
      "Promoter normalization (global per channel):\n",
      "  Mean channel: μ=1.015, σ=5.113\n",
      "  Max channel:  μ=2.123, σ=8.314\n",
      "\n",
      "Target normalization:\n",
      "  Original range: [0.0, 19519.8]\n",
      "  Log range: [0.000, 9.879]\n",
      "  Normalized range: [-0.653, 4.481]\n",
      "\n",
      "Dataset size: 32568 samples\n",
      "Tensor shape: (32568, 14, 190)\n",
      "============================================================\n",
      "\n",
      "\n",
      "Step 2: Creating test dataset with training normalization...\n",
      "\n",
      "============================================================\n",
      "TEST MODE: Applying training normalization parameters\n",
      "============================================================\n",
      "\n",
      "Using training normalization stats:\n",
      "  Mean channel: μ=1.015, std=5.113\n",
      "  Max channel:  μ=2.123, std=8.314\n",
      "\n",
      "Dataset size: 1984 samples\n",
      "Tensor shape: (1984, 14, 190)\n",
      "============================================================\n",
      "\n",
      "\n",
      "Step 3: Loading trained model...\n",
      "\n",
      "Step 4: Making predictions...\n",
      "\n",
      "Step 5: Denormalizing predictions...\n",
      "\n",
      "============================================================\n",
      "PREDICTION SUMMARY\n",
      "============================================================\n",
      "  Shape: (1984,)\n",
      "  Range: [0.000, 172654288.607]\n",
      "  Mean:  273655.766\n",
      "  Median: 0.000\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = None\n",
    "test_genes = cell_line_x3.gene_annotations[\"gene_name\"].values\n",
    "\n",
    "print(f\"\\nPredicting on {len(test_genes)} test genes\\n\")\n",
    "\n",
    "# Create training dataset (computes normalization)\n",
    "print(\"Step 1: Loading training data and computing normalization...\")\n",
    "train_ds = GeneExpressionDataset(combined_features)\n",
    "\n",
    "# Get normalization params\n",
    "norm_params = train_ds.get_normalization_params()\n",
    "\n",
    "# Create test dataset (applies training normalization)\n",
    "print(\"\\nStep 2: Creating test dataset with training normalization...\")\n",
    "test_ds = GeneExpressionDataset(cell_line_x3, normalize_params=norm_params)\n",
    "\n",
    "# Create and load model\n",
    "print(\"\\nStep 3: Loading trained model...\")\n",
    "model = PromoterAttentionCNN(\n",
    "    n_bins=combined_features.n_total_bins,\n",
    "    n_features=N_FEATURES,\n",
    "    n_channels=N_CHANNELS,\n",
    ")\n",
    "\n",
    "net = NeuralNetRegressor(\n",
    "    model,\n",
    "    device=DEVICE.type,\n",
    "    batch_size=BATCH_SIZE * 2,\n",
    ")\n",
    "\n",
    "net.initialize()\n",
    "net.load_params(f_params=\"/workspaces/Gene-Expression-Prediction/src/gene_expression_prediction/checkpoints_promoter_only/best_model.pt\")\n",
    "\n",
    "# Predict (returns normalized predictions)\n",
    "print(\"\\nStep 4: Making predictions...\")\n",
    "pred_normalized = net.predict(test_ds)\n",
    "\n",
    "# Denormalize back to original scale\n",
    "print(\"\\nStep 5: Denormalizing predictions...\")\n",
    "pred = train_ds.denormalize_targets(pred_normalized)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"PREDICTION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Shape: {pred.shape}\")\n",
    "print(f\"  Range: [{pred.min():.3f}, {pred.max():.3f}]\")\n",
    "print(f\"  Mean:  {pred.mean():.3f}\")\n",
    "print(f\"  Median: {np.median(pred):.3f}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "# Validation\n",
    "assert isinstance(pred, np.ndarray), \"Prediction must be numpy array\"\n",
    "assert np.issubdtype(pred.dtype, np.number), \"Prediction must be numeric\"\n",
    "assert pred.shape[0] == len(test_genes), \"One prediction per gene\"\n",
    "assert not np.isnan(pred).any(), \"No NaN values\"\n",
    "assert not np.isinf(pred).any(), \"No Inf values\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Predictions in the Required Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: /workspaces/Gene-Expression-Prediction/data/output/Tokar_David_Project1.zip\n",
      "\n",
      "Preview of the first 5 rows of the submission file:\n",
      "    gene_name  gex_predicted\n",
      "0       CAPN9            0.0\n",
      "1        ILF2            0.0\n",
      "2  ST6GALNAC5            0.0\n",
      "3  MROH7-TTC4            0.0\n",
      "4        AGO4            0.0\n"
     ]
    }
   ],
   "source": [
    "# Store predictions in a ZIP.\n",
    "# Upload this zip on the project website under \"Your submission\".\n",
    "# Zip this notebook along with the conda environment (and README, optional) and upload this under \"Your code\".\n",
    "save_dir = \"/workspaces/Gene-Expression-Prediction/data/output\"\n",
    "file_name = \"gex_predicted.csv\"  # PLEASE DO NOT CHANGE THIS\n",
    "zip_name = \"Tokar_David_Project1.zip\"\n",
    "save_path = f\"{save_dir}/{zip_name}\"\n",
    "compression_options = {\"method\": \"zip\", \"archive_name\": file_name}\n",
    "\n",
    "submission_df = pd.DataFrame({\"gene_name\": test_genes, \"gex_predicted\": pred})\n",
    "\n",
    "compression_options = {\"method\": \"zip\", \"archive_name\": file_name}\n",
    "\n",
    "submission_df.to_csv(save_path, index=False, compression=compression_options)\n",
    "print(f\"File saved to: {save_path}\")\n",
    "print(\"\\nPreview of the first 5 rows of the submission file:\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
